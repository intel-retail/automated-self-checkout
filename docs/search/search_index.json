{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Intel\u00ae Automated Self-Checkout Reference Package","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>As Computer Vision becomes more and more mainstream, especially for industrial &amp; retail use cases, development and deployment of these solutions becomes more challenging. Vision workloads are large and complex and need to go through many stages. For instance, in the pipeline below, the video data is ingested, pre-processed before each inferencing step, inferenced using two models - YOLOv5 and EfficientNet, and post processed to generate metadata and show the bounding boxes for each frame. This pipeline is just an example of the supported models and pipelines found within this reference.</p> <p></p> <p>Automated self-checkout solutions are complex, and retailers, independent software vendors (ISVs), and system integrators (SIs) require a good understanding of hardware and software, the costs involved in setting up and scaling the system, and the configuration that best suits their needs. Vision workloads are significantly larger and require systems to be architected, built, and deployed with several considerations. Hence, a set of ingredients needed to create an automated self-checkout solution is necessary. More details are available on the Intel Developer Focused Webpage and on this LinkedIn Blog</p> <p>The Intel\u00ae Automated Self-Checkout Reference Package provides critical components required to build and deploy a self-checkout use case using Intel\u00ae hardware, software, and other open-source software. This reference implementation provides a pre-configured automated self-checkout pipeline that is optimized for Intel\u00ae hardware. The solution includes profiles and optimization using Open Vino Model Server (OVMS) as shown in the figure below. </p> <p>The reference solution also includes a set of benchmarking tools, shown in the image below, to evaluate the workload on different hardware platforms. This reference solution will help evaluate your required hardware to minimize the cost per workload.</p> <p></p>"},{"location":"index.html#install-platform","title":"Install Platform","text":"<p>Make sure that your platform is included in the supported platform list. To set up the platform, refer to Hardware Setup.</p>"},{"location":"index.html#releases","title":"Releases","text":"<p>For the project release notes, refer to the GitHub* Repository.</p>"},{"location":"index.html#license","title":"License","text":"<p>This project is Licensed under an Apache License.</p>"},{"location":"LICENSE.html","title":"LICENSE","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li>Definitions.</li> </ol> <p>\"License\" shall mean the terms and conditions for use, reproduction,    and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by    the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all    other entities that control, are controlled by, or are under common    control with that entity. For the purposes of this definition,    \"control\" means (i) the power, direct or indirect, to cause the    direction or management of such entity, whether by contract or    otherwise, or (ii) ownership of fifty percent (50%) or more of the    outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity    exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,    including but not limited to software source code, documentation    source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical    transformation or translation of a Source form, including but    not limited to compiled object code, generated documentation,    and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or    Object form, made available under the License, as indicated by a    copyright notice that is included in or attached to the work    (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object    form, that is based on (or derived from) the Work and for which the    editorial revisions, annotations, elaborations, or other modifications    represent, as a whole, an original work of authorship. For the purposes    of this License, Derivative Works shall not include works that remain    separable from, or merely link (or bind by name) to the interfaces of,    the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including    the original version of the Work and any modifications or additions    to that Work or Derivative Works thereof, that is intentionally    submitted to Licensor for inclusion in the Work by the copyright owner    or by an individual or Legal Entity authorized to submit on behalf of    the copyright owner. For the purposes of this definition, \"submitted\"    means any form of electronic, verbal, or written communication sent    to the Licensor or its representatives, including but not limited to    communication on electronic mailing lists, source code control systems,    and issue tracking systems that are managed by, or on behalf of, the    Licensor for the purpose of discussing and improving the Work, but    excluding communication that is conspicuously marked or otherwise    designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity    on behalf of whom a Contribution has been received by Licensor and    subsequently incorporated within the Work.</p> <ol> <li> <p>Grant of Copyright License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    copyright license to reproduce, prepare Derivative Works of,    publicly display, publicly perform, sublicense, and distribute the    Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    (except as stated in this section) patent license to make, have made,    use, offer to sell, sell, import, and otherwise transfer the Work,    where such license applies only to those patent claims licensable    by such Contributor that are necessarily infringed by their    Contribution(s) alone or by combination of their Contribution(s)    with the Work to which such Contribution(s) was submitted. If You    institute patent litigation against any entity (including a    cross-claim or counterclaim in a lawsuit) alleging that the Work    or a Contribution incorporated within the Work constitutes direct    or contributory patent infringement, then any patent licenses    granted to You under this License for that Work shall terminate    as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the    Work or Derivative Works thereof in any medium, with or without    modifications, and in Source or Object form, provided that You    meet the following conditions:</p> </li> </ol> <p>(a) You must give any other recipients of the Work or    Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices    stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works    that You distribute, all copyright, patent, trademark, and    attribution notices from the Source form of the Work,    excluding those notices that do not pertain to any part of    the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its    distribution, then any Derivative Works that You distribute must    include a readable copy of the attribution notices contained    within such NOTICE file, excluding those notices that do not    pertain to any part of the Derivative Works, in at least one    of the following places: within a NOTICE text file distributed    as part of the Derivative Works; within the Source form or    documentation, if provided along with the Derivative Works; or,    within a display generated by the Derivative Works, if and    wherever such third-party notices normally appear. The contents    of the NOTICE file are for informational purposes only and    do not modify the License. You may add Your own attribution    notices within Derivative Works that You distribute, alongside    or as an addendum to the NOTICE text from the Work, provided    that such additional attribution notices cannot be construed    as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and    may provide additional or different license terms and conditions    for use, reproduction, or distribution of Your modifications, or    for any such Derivative Works as a whole, provided Your use,    reproduction, and distribution of the Work otherwise complies with    the conditions stated in this License.</p> <ol> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,    any Contribution intentionally submitted for inclusion in the Work    by You to the Licensor shall be under the terms and conditions of    this License, without any additional terms or conditions.    Notwithstanding the above, nothing herein shall supersede or modify    the terms of any separate license agreement you may have executed    with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade    names, trademarks, service marks, or product names of the Licensor,    except as required for reasonable and customary use in describing the    origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or    agreed to in writing, Licensor provides the Work (and each    Contributor provides its Contributions) on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or    implied, including, without limitation, any warranties or conditions    of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A    PARTICULAR PURPOSE. You are solely responsible for determining the    appropriateness of using or redistributing the Work and assume any    risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,    whether in tort (including negligence), contract, or otherwise,    unless required by applicable law (such as deliberate and grossly    negligent acts) or agreed to in writing, shall any Contributor be    liable to You for damages, including any direct, indirect, special,    incidental, or consequential damages of any character arising as a    result of this License or out of the use or inability to use the    Work (including but not limited to damages for loss of goodwill,    work stoppage, computer failure or malfunction, or any and all    other commercial damages or losses), even if such Contributor    has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing    the Work or Derivative Works thereof, You may choose to offer,    and charge a fee for, acceptance of support, warranty, indemnity,    or other liability obligations and/or rights consistent with this    License. However, in accepting such obligations, You may act only    on Your own behalf and on Your sole responsibility, not on behalf    of any other Contributor, and only if You agree to indemnify,    defend, and hold each Contributor harmless for any liability    incurred by, or claims asserted against, such Contributor by reason    of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2023 Intel Corporation</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"faq.html","title":"Frequently Asked Questions","text":""},{"location":"faq.html#what-are-the-platform-requirements","title":"What are the platform requirements?","text":"<p>For optimal hardware, refer to the platform guide.</p>"},{"location":"faq.html#what-are-the-software-prerequisites","title":"What are the software prerequisites?","text":"<p>At a minimum, you will need Docker* 23.0 or later. For more details, refer to the hardware setup.</p>"},{"location":"faq.html#how-do-i-download-the-models","title":"How do I download the models?","text":"<p>The models are downloaded automatically when the benchmark script <code>benchmark.sh</code> is run. For more details on downloading models, refer to Pipeline Setup.</p>"},{"location":"faq.html#how-do-i-simulate-rtsp-cameras","title":"How do I simulate RTSP cameras?","text":"<p>You can use the camera simulator script <code>camera-simulator.sh</code> to run simulated RTSP camera streams. For more details on the script, refer to Run Camera Simulator page.</p>"},{"location":"faq.html#how-do-i-download-video-files-for-benchmarking","title":"How do I download video files for benchmarking?","text":"<p>You can download your own media file or use the the provided <code>download_sample_videos.sh</code> script to download an existing media file. For more details, refer to Benchmark Pipeline.</p>"},{"location":"faq.html#how-do-i-run-different-types-of-pipelines","title":"How do I run different types of pipelines?","text":"<p>For details on running different types of pipelines, refer to Quick Start Guide.</p>"},{"location":"faq.html#where-are-the-pipeline-container-logs-located","title":"Where are the pipeline container logs located ?","text":"<p>Pipeline container logs are redirected to a text file at <code>automated-self-checkout/results</code>. For every new pipeline a new log file <code>pipeline##.log</code> (eg.: pipeline0.log, pipeline1.log, ... etc.)  is created.</p> <p>Note</p> <p>As pipeline container logs are redirected to separate text files, docker logs displayed using portainer or command <code>docker logs &lt;**CONTAINER**&gt;</code> will be empty.</p>"},{"location":"hardwaresetup.html","title":"System Setup","text":""},{"location":"hardwaresetup.html#prerequisites","title":"Prerequisites","text":"<p>To build the Intel\u00ae Automated Self-Checkout Reference Package, you need:</p> <ul> <li>Ubuntu LTS (22.04 or 20.04)</li> <li>Docker (Tested on version &gt;= 23.0.0)</li> <li>Docker Compose v2 (Required, if using docker compose feature)</li> <li>Git</li> </ul> <p>Click the links for corresponding set up instructions.</p>"},{"location":"hardwaresetup.html#hardware-dependent-installation","title":"Hardware Dependent Installation","text":"11th/12th Gen Intel\u00ae Core\u2122 ProcessorIntel\u00ae Xeon\u00ae ProcessorIntel\u00ae Data Center GPU Flex 140/170Intel\u00ae Arc\u2122 <ol> <li> <p>Download Ubuntu 20.04 and follow these installation steps.</p> </li> <li> <p>Install Docker* Engine</p> <p>Note</p> <p>To avoid typing <code>sudo</code> when running the Docker command, follow these steps.</p> </li> <li> <p>[Optional] Install Docker Compose v2, if using the docker compose feature</p> </li> <li> <p>Install Git</p> </li> <li> <p>Set up the pipeline</p> </li> </ol> <ol> <li> <p>Download Ubuntu 22.04 and follow these installation steps.</p> </li> <li> <p>Install Docker Engine</p> </li> <li> <p>[Optional] Install Docker Compose v2, if using the docker compose feature</p> </li> <li> <p>Install Git</p> </li> <li> <p>Set up the pipeline</p> </li> </ol> <ol> <li> <p>Download Ubuntu 22.04 and follow these installation steps.</p> </li> <li> <p>Update the Kernel</p> <p>Warning</p> <p>After the kernel is updated, <code>apt-get install</code> might not work due to the unsupported kernel versions that were installed. To resolve this issue, do the following:</p> <ol> <li> <p>Find all the installed kernels</p> <pre><code>    dpkg --list | grep -E -i --color 'linux-image|linux-headers'\n</code></pre> </li> <li> <p>Then remove the unsupported kernels. The example below will remove the installed kernel 5.19:</p> <pre><code>    sudo apt-get purge -f 'linux--5.19'\n</code></pre> </li> </ol> </li> <li> <p>Install Docker Engine</p> </li> <li> <p>[Optional] Install Docker Compose v2, if using the docker compose feature</p> </li> <li> <p>Install Git</p> </li> <li> <p>Set up the pipeline</p> </li> </ol> <ol> <li> <p>Download Ubuntu 20.04 and follow these installation steps.</p> </li> <li> <p>Update the Kernel</p> <p>Warning</p> <p>After the kernel is updated, <code>apt-get install</code> might not work due to the unsupported kernel versions that were installed. To resolve this issue, do the following:</p> <ol> <li> <p>Find all the installed kernels</p> <pre><code>    dpkg --list | grep -E -i --color 'linux-image|linux-headers'\n</code></pre> </li> <li> <p>Then remove the unsupported kernels. The example below will remove the installed kernel 5.19:</p> <pre><code>    sudo apt-get purge -f 'linux--5.19'\n</code></pre> </li> </ol> </li> <li> <p>Install Docker Engine</p> </li> <li> <p>[Optional] Install Docker Compose v2, if using the docker compose feature</p> </li> <li> <p>Install Git</p> </li> <li> <p>Set up the pipeline</p> </li> </ol>"},{"location":"platforms.html","title":"Supported Platforms","text":"<p>Following are the list of supported platforms:</p>"},{"location":"platforms.html#cpu","title":"CPU","text":"<ul> <li> <p>11th Gen Intel\u00ae Core\u2122 i5 Processor/11th Gen Intel\u00ae Core\u2122 i7 Processor</p> </li> <li> <p>12th Gen Intel\u00ae Core\u2122 i5 Processor/12th Gen Intel\u00ae Core\u2122 i7 Processor</p> </li> <li> <p>Intel\u00ae Xeon\u00ae Platinum 8351N Processor</p> </li> </ul>"},{"location":"platforms.html#gpu","title":"GPU","text":"<ul> <li> <p>Intel\u00ae Data Center GPU Flex 140</p> </li> <li> <p>Intel\u00ae Data Center GPU Flex 170</p> </li> <li> <p>Intel\u00ae Arc\u2122 A770M Graphics</p> </li> </ul> <p>To set up the platform, refer to Hardware Setup.</p>"},{"location":"query_usb_camera.html","title":"Query USB Camera","text":"<ol> <li> <p>Make sure a USB camera is plugged into the system</p> </li> <li> <p>Install the necessary libraries</p> <pre><code>sudo apt update\nsudo apt install v4l-utils -y\n</code></pre> </li> <li> <p>List available video cameras </p> <pre><code>ls -l /dev/vid*\n</code></pre> <p>Note</p> <p>To get information about the development video ids, check the </p> </li> <li> <p>Execute a video, from the available list, for more information</p> <pre><code>v4l2-ctl --list-formats-ext -d /dev/video0\n</code></pre> <p>Note</p> <p>Here is information on how to .</p> </li> </ol> <p>Example</p> <p>Here is an example to run the pipeline with a USB camera on video0 for the core system: <pre><code>sudo ./run.sh --platform core --inputsrc /dev/video0\n</code></pre></p>"},{"location":"releasenotes.html","title":"Releases","text":"<p>Release v1.0.1</p> <p>Release v1.5.0</p> <p>Release v2.0.0</p>"},{"location":"roadmap.html","title":"Roadmap","text":"Roadmap Timeline Notes Now <ul> <li>Computer Vision Pipeline             A single service that processes a streaming video and runs inferencing across products placed on a self-checkout station.</li> <li>Benchmark Script           A script that allows the end user to reproduce our benchmark results. The script provides capabilities to run benchmarks on partner models.</li> <li>Benchmark Results           Detailed benchmark results across GPU, CPU and GPU hardware SKUs</li> </ul> <p>Context</p> <p>Automation is rapidly becoming the key transformational strategy for retailers. Age old problems such as inventory control, planogram compliance, store security, store operations, warehousing are areas that can benefit from automation that is predominantly driven by computer vision and AI. Whilst computer vision-based solutions exist for many of these problems, not many have moved beyond pilots and scaled across supermarkets due to various technical and business reasons. We are looking to address the use of computer vision across a multiple set of use cases in the retail space. The first of these use cases will be to help address the scalability of vision enabled self-checkout solutions. This will be followed by use cases such as Loss Prevention, AI assisted Shopping Carts, Autonomous Stores and many more in the future as the retail landscape evolves. The goal of this roadmap is to provide key ingredients and easy decision making to our partners on their journey to build and deploy these use cases at scale.</p> <p>High-level Focus</p>         Vision enabled use cases will need to address four fundamental areas to build and deploy solutions at scale.         <ul> <li>Camera Management           Cameras are a critical piece of the infrastructure, providing both video streams and images to be used across these use cases. We will be focusing on providing a consistent mechanism of onboarding different types of cameras and managing its lifecycle.</li> <li>Computer Vision Pipeline           Workload deployment options and choices of frameworks makes the vision pipeline complex. We will focus our breaking down the pipeline into its individual services and making it easier to deploy and run across distributed architecture.</li> <li>Hardware Recommendation           There are many unknowns when moving from a pilot to production and especially during the scale phase of the project. For every use case, it is important to know exactly what infrastructure is required to deploy and scale the solution.           We plan to remove all the guess work around what hardware is required to run these workloads.</li> <li> Deployment with ISVs            The time taken to operationalize new AI-based software in a new environment is often long. Working with our partners, we would focus our efforts in reducing the time to production.</li> </ul> <p>Out-of-Scope</p>       There are several items that will not be considered as part of this reference implementation. This is not an exhaustive list, but these are core exclusions as they are not our differentiators:       <ul> <li>We will not build or recommend AI models</li> <li>We will not build end-to-end solutions</li> <li>We will not advocate for a specific software deployment architecture</li> </ul> Next <ul> <li>Distributed Architecture         Separating out media pre-processing, AI inferencing and post-processing as completely independent services. Providing a mechanism to deploy and run the individual services across distributed heterogeneous compute. Publish all events to an Enterprise Service Bus (ESB).</li> <li>Update Benchmark script         Update the script to reflect the new distributed architecture.</li> <li>Benchmark Results         Benchmark results across CPU and GPU and GPU only hardware SKUs (previous results updated and new SKUs added)</li> </ul> Later <ul> <li>Model Drift and updates         Monitor model accuracy and rectify model drift. Add new models in live systems.</li> <li>Edge Training         Provide a mechanism to do localized instore training to ensure new products are identified without delay.</li> <li>Hierarchical model support         Provide a mechanism to have a hierarchy of models that gets chosen and executed based on initial segmentation.</li> <li>Dev Cloud Support         Provide a cloud environment for partners to access the hardware and run benchmarks.</li> <li>Camera Management         Provide a mechanism to onboard and drive the camera lifecycle across the store.</li> </ul> <p>Disclaimer: The roadmap is for informational purposes only and is subject to change.</p> <p>Help Drive Our Roadmap Priorities We want to drive our roadmap by building the most valuable assets/ingredients to our partners. Our roadmap will be public and will provide complete transparency to ensure we receive continuous feedback from our partners. Raise any issues and requirements to help us guide our roadmap priorities, and in doing so we can drive the retail vertical forward. </p> <p>Open an issue on GitHub to report a problem or to provide feedback.</p>"},{"location":"troubleshooting.html","title":"Troubleshooting","text":"<p>Q: Why is the performance sometimes on CPU better than on GPU, when running pipeline benchmarking like stream density ?</p> <p>A: The performance of pipeline benchmarking strongly depends on the models.  Specifically for <code>yolov5s</code> object detection, it is recommended to use the model precision FP32 when it is running on device <code>GPU</code>.  If supported, then you can change the model precision by going to the folder <code>configs/opencv-ovms/models/2022</code> from the root of project folder and editing the <code>base_path</code> for that particular model in the <code>config_template.json</code> file.  For example, you can change the the base_path of <code>FP16</code> to <code>FP32</code> assuming the precision <code>FP32</code> of the model yolov5s is available:  </p> <pre><code>    ...\n    \"config\": {\n    \"name\": \"yolov5s\",\n    \"base_path\": \"/models/yolov5s/FP32-INT8\",\n    \"layout\": \"NHWC:NCHW\",\n    ...\n    }\n</code></pre>"},{"location":"webcam_rtsp.html","title":"Webcam to RTSP","text":"<p>When using /dev/video0 as input, only one container can use the webcam at the time. If the intention is to run multiple pipelines at once using a webcam as the input, then, the solution is to covert the webcam to an RTSP path.</p> <p>Run:</p> <pre><code>make webcam-rtsp\n</code></pre> <p>Then use the path rtsp://127.0.0.1:8554/cam as input</p>"},{"location":"OVMS/camera_serial_number.html","title":"Get Serial Number of Intel\u00ae RealSense\u2122 Camera","text":"<p>Do the following to get the serial number of an Intel\u00ae RealSense\u2122 Camera:</p> <ol> <li>Build the RealSense version of dlstreamer Docker image if not done yet:</li> </ol> <pre><code>make build-dlstreamer-realsense\n</code></pre> <ol> <li> <p>Plug in your Intel\u00ae RealSense\u2122 Camera into the system;</p> </li> <li> <p>Use the makefile target <code>get-realsense-serial-num</code> to get the serial number of your Intel\u00ae RealSense\u2122 Camera:</p> </li> </ol> <pre><code>make get-realsense-serial-num\n</code></pre> <p>You should see a serial number printed out. If you do not see the expected results, check if the Intel\u00ae RealSense\u2122 Camera is plugged in.</p>"},{"location":"OVMS/capiPipelineRun.html","title":"OpenVINO OVMS C-API Pipeline Run","text":"<p>OpenVINO Model Server has many ways to run inferencing pipeline: TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here is a demonstration for using OVMS C API method to run face detection inferencing pipeline with steps below:</p> <ol> <li>Add new section to model configuration file for model server</li> <li>Add pipeline specific files</li> <li>Add environment variable file dependency</li> <li>Add a profile launcher pipeline configuration file</li> <li>Build and run</li> </ol>"},{"location":"OVMS/capiPipelineRun.html#add-new-section-to-model-config-file-for-model-server","title":"Add New Section To Model Config File for Model Server","text":"<p>Here is the template config file location: <code>configs/opencv-ovms/models/2022/config_template.json</code>, edit the file and append the following face detection model configuration section to the template <pre><code>,\n{\"config\": {\n      \"name\": \"face-detection-retail-0005\",\n      \"base_path\": \"face-detection-retail-0005/FP16-INT8\",\n      \"shape\": \"(1,3,800,800)\",\n      \"nireq\": 2,\n      \"batch_size\":\"1\",\n      \"plugin_config\": {\"PERFORMANCE_HINT\": \"LATENCY\"},\n      \"target_device\": \"{target_device}\"},\n      \"latest\": { \"num_versions\": 2 }\n    }\n</code></pre></p> <p>Note</p> <p><code>shape</code> is optional and takes precedence over batch_size, please remove this attribute if you don't know the value for the model.</p> <p>Note</p> <p>Please leave <code>target_device</code> value as it is, as the value <code>{target_device}</code> will be recognized and replaced by script run.</p> <p>You can find the parameter description in the ovms docs.</p>"},{"location":"OVMS/capiPipelineRun.html#add-pipeline-specific-files","title":"Add pipeline specific files","text":"<p>Here is the list of files we added in directory of <code>configs/opencv-ovms/gst_capi/pipelines/face_detection/</code>:</p> <ol> <li><code>main.cpp</code> - this is all the work about pre-processing before sending to OVMS for inferencing and post-processing for displaying.</li> <li><code>Makefile</code> - to help building the pre-processing and post-processing binary.</li> </ol>"},{"location":"OVMS/capiPipelineRun.html#add-environment-variable-file","title":"Add Environment Variable File","text":"<p>You can add multiple environment variable files to <code>configs/opencv-ovms/envs/</code> directory for your pipeline. For face detection pipeline run, we have added <code>configs/opencv-ovms/envs/capi_face_detection.env</code> environment variable file. Below is a list of explanation for all environment variables and current default values we set for face detection pipeline run, this list can be extended for any future modification.</p> EV Name Face Detection Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU &amp; GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/face_detection/face_detection pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.9 detection threshold value in floating point that needs to be between 0.0 to 1.0 <p>Note</p> <p>Details of the face detection pipeline environment variable file can be viewed in <code>configs/opencv-ovms/envs/capi_face_detection.env</code>.</p>"},{"location":"OVMS/capiPipelineRun.html#add-a-profile-launcher-configuration-file","title":"Add A Profile Launcher Configuration File","text":"<p>The details about Profile Launcher configuration can be found here, and the details of capi face detection profile launcher configuration can be viewed in <code>configs/opencv-ovms/cmd_client/res/capi_face_detection/configuration.yaml</code>.</p>"},{"location":"OVMS/capiPipelineRun.html#build-and-run","title":"Build and Run","text":"<p>Here are the quick start steps to build and run OVMS C API face detection pipeline profile:</p> <ol> <li> <p>Build gst-capi ovms with profile-launcher: </p> <pre><code>make build-capi_face_detection\n</code></pre> </li> <li> <p>Download sample video files: </p> <pre><code>cd benchmark-scripts/ &amp;&amp; ./download_sample_videos.sh &amp;&amp; cd ..\n</code></pre> </li> <li> <p>Start simulator camera: </p> <pre><code>make run-camera-simulator\n</code></pre> </li> <li> <p>To start face detection pipeline:</p> <pre><code>PIPELINE_PROFILE=\"capi_face_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0. 1:8554/camera_0\n</code></pre> <p>Note</p> <p>The pipeline run will automatically download the OpenVINO model files listed in <code>configs/opencv-ovms/models/2022/config_template.json</code></p> </li> </ol>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html","title":"OpenVINO OVMS C-API Yolov5 Ensemble Pipeline Run","text":"<p>OpenVINO Model Server has many ways to run inferencing pipeline: TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here we are demonstrating for using OVMS C API method to run inferencing pipeline yolov5s ensemble models in following steps:</p> <ol> <li>Add new section to model configuration file for model server</li> <li>Add pipeline specific files</li> <li>Add environment variable file dependency</li> <li>Add a profile launcher pipeline configuration file</li> <li>Build and run</li> <li>Clean up</li> </ol>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-new-section-to-model-config-file-for-model-server","title":"Add New Section To Model Config File for Model Server","text":"<p>The model template configuration file has been updated with model configs of yolov5, efficientnetb0_FP32INT8 and custom configurations, please view <code>configs/opencv-ovms/models/2022/config_template.json</code> for detail.</p> <p>Note</p> <p>New model yolov5 is similar to yolov5s configuration except the layout difference.</p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-pipeline-specific-files","title":"Add pipeline specific files","text":"<p>The pre-processing and post-processing work files are added in directory of <code>configs/opencv-ovms/gst_capi/pipelines/capi_yolov5_ensemble/</code>, please view directory for details.</p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-environment-variable-file","title":"Add Environment Variable File","text":"<p>You can add multiple environment variable files to <code>configs/opencv-ovms/envs/</code> directory for your pipeline, we've added <code>capi_yolov5_ensemble.env</code> for yolov5 ensemble pipeline run. Below is a list of explanation for all environment variables and current default values we set, this list can be extended for any future modification.</p> EV Name Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU &amp; GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/capi_yolov5_ensemble/capi_yolov5_ensemble pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.7 detection threshold value in floating point that needs to be between 0.0 to 1.0 BARCODE 1 For capi_yolov5_ensemble pipeline, you can enable barcode detection. value: 0 or 1 <p>details of yolov5s pipeline environment variable file can be viewed in <code>configs/opencv-ovms/envs/capi_yolov5_ensemble.env</code>.</p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-a-profile-launcher-configuration-file","title":"Add A Profile Launcher Configuration File","text":"<p>The details about Profile Launcher configuration can be found here, details for yolov5 pipeline profile launcher configuration can be viewed in <code>configs/opencv-ovms/cmd_client/res/capi_yolov5_ensemble/configuration.yaml</code></p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#build-and-run","title":"Build and Run","text":"<p>Here are the quick start steps to build and run capi yolov5 pipeline profile :</p> <ol> <li>Build docker image with profile-launcher: <code>make build-capi_yolov5_ensemble</code></li> <li>Download sample video files: <code>cd benchmark-scripts/ &amp;&amp; ./download_sample_videos.sh &amp;&amp; cd ..</code></li> <li>Start simulator camera: <code>make run-camera-simulator</code></li> <li>To start the pipeline run: <code>PIPELINE_PROFILE=\"capi_yolov5_ensemble\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0</code></li> </ol> <p>Note</p> <p>The pipeline will automatically download the OpenVINO model files listed in <code>configs/opencv-ovms/models/2022/config_template.json</code></p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#clean-up","title":"Clean Up","text":"<p>To stop existing container: <code>make clean-capi_yolov5_ensemble</code> To stop all running containers including camera simulator and remove all log files: <code>make clean-all</code></p>"},{"location":"OVMS/capiYolov5PipelineRun.html","title":"OpenVINO OVMS C-API Yolov5 Pipeline Run","text":"<p>OpenVINO Model Server has many ways to run inferencing pipeline: TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here we are demonstrating for using OVMS C API method to run inferencing pipeline yolov5s model in following steps:</p> <ol> <li>Add new section to model configuration file for model server</li> <li>Add pipeline specific files</li> <li>Add environment variable file dependency</li> <li>Add a profile launcher pipeline configuration file</li> <li>Build and run</li> </ol>"},{"location":"OVMS/capiYolov5PipelineRun.html#add-new-section-to-model-config-file-for-model-server","title":"Add New Section To Model Config File for Model Server","text":"<p>Here is the template config file location: <code>configs/opencv-ovms/models/2022/config_template.json</code>, edit the file and append the new model's configuration into the template, such as yolov5 model as shown below: <pre><code>    {\n      \"config\": {\n        \"name\": \"yolov5s\",\n        \"base_path\": \"/models/yolov5s/FP16-INT8\",\n        \"layout\": \"NHWC:NCHW\",\n        \"shape\": \"(1,416,416,3)\",\n        \"nireq\": 1,\n        \"batch_size\": \"1\",\n        \"plugin_config\": {\n          \"PERFORMANCE_HINT\": \"LATENCY\"\n        },\n        \"target_device\": \"{target_device}\"\n      }\n    }\n</code></pre></p> <p>Note</p> <p><code>shape</code> is optional and takes precedence over batch_size, please remove this attribute if you don't know the value for the model.</p> <p>Note</p> <p>Please leave <code>target_device</code> value as it is, as the value <code>{target_device}</code> will be recognized and replaced by script run.</p> <p>You can find the parameter description in the ovms docs.</p>"},{"location":"OVMS/capiYolov5PipelineRun.html#add-pipeline-specific-files","title":"Add pipeline specific files","text":"<p>Here is the list of files we added in directory of <code>configs/opencv-ovms/gst_capi/pipelines/capi_yolov5/</code>:</p> <ol> <li><code>main.cpp</code> - this is all the work about pre-processing before sending to OVMS for inferencing and post-processing for displaying.</li> <li><code>Makefile</code> - to help building the pre-processing and post-processing binary.</li> </ol>"},{"location":"OVMS/capiYolov5PipelineRun.html#add-environment-variable-file","title":"Add Environment Variable File","text":"<p>You can add multiple environment variable files to <code>configs/opencv-ovms/envs/</code> directory for your pipeline, we've added <code>capi_yolov5.env</code> for yolov5 pipeline run. Below is a list of explanation for all environment variables and current default values we set, this list can be extended for any future modification.</p> EV Name Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU &amp; GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/capi_yolov5/capi_yolov5 pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.7 detection threshold value in floating point that needs to be between 0.0 to 1.0 <p>details of yolov5s pipeline environment variable file can be viewed in <code>configs/opencv-ovms/envs/capi_yolov5.env</code>.</p>"},{"location":"OVMS/capiYolov5PipelineRun.html#add-a-profile-launcher-configuration-file","title":"Add A Profile Launcher Configuration File","text":"<p>The details about Profile Launcher configuration can be found here, details for yolov5 pipeline profile launcher configuration can be viewed in <code>configs/opencv-ovms/cmd_client/res/capi_yolov5/configuration.yaml</code></p>"},{"location":"OVMS/capiYolov5PipelineRun.html#build-and-run","title":"Build and Run","text":"<p>Here are the quick start steps to build and run capi yolov5 pipeline profile :</p> <ol> <li>Build docker image with profile-launcher: <code>make build-capi_yolov5</code></li> <li>Download sample video files: <code>cd benchmark-scripts/ &amp;&amp; ./download_sample_videos.sh &amp;&amp; cd ..</code></li> <li>Start simulator camera: <code>make run-camera-simulator</code></li> <li>To start the pipeline run: <code>PIPELINE_PROFILE=\"capi_yolov5\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1</code></li> </ol> <p>Note</p> <p>The pipeline will automatically download the OpenVINO model files listed in <code>configs/opencv-ovms/models/2022/config_template.json</code></p>"},{"location":"OVMS/pipelineDockerCompose.html","title":"Docker-Compose for Developer Toolbox","text":"<p>Pipelines can be run using docker-compose files. Changes are custom made inside the <code>docker-compose.yml</code> file for integration with the Developer Toolbox.</p> <p>Note</p> <p>To utilize all the features offered by Automated Self-Checkout, run the pipelines as illustrated in the section Run Pipelines.</p>"},{"location":"OVMS/pipelineDockerCompose.html#steps-to-run-pipelines","title":"Steps to Run Pipelines","text":"<ol> <li> <p>Before running, complete the prerequisites for setting Up the Pipelines.     !!! Note         Ensure Docker Compose v2 is installed in order to run the pipelines via this feature. </p> </li> <li> <p>Customize the <code>docker-compose.yml</code> to add the number of camera simulators required and the number of different type of pipelines that need to be run</p> <p>Note</p> <p>Follow all the instructions in <code>docker-compose.yml</code> for customizations.</p> </li> <li> <p>Run the pipelines</p> <pre><code>make run-pipelines\n</code></pre> </li> <li> <p>All the containers i.e camera simulators, OVMS server and pipelines should start without any errors in portainer as shown below in Figure 1</p> <p> Figure 1: Pipelines Running Successfully </p> </li> <li> <p>Stop the pipelines</p> <pre><code>make down-pipelines\n</code></pre> </li> </ol>"},{"location":"OVMS/pipelinebenchmarking.html","title":"Computer Vision Pipeline Benchmarking","text":"<p>You can benchmark pipelines with a collection of scripts to get the pipeline performance metrics such as video processing in frame-per-second (FPS), memory usage, power consumption, and so on.</p>"},{"location":"OVMS/pipelinebenchmarking.html#prerequisites","title":"Prerequisites","text":"<p>Before benchmarking, make sure you set up the pipeline.</p>"},{"location":"OVMS/pipelinebenchmarking.html#steps-to-benchmark-computer-vision-pipelines","title":"Steps to Benchmark Computer Vision Pipelines","text":"<ol> <li> <p>Build the benchmark Docker* images     Benchmark scripts are containerized inside Docker. The following table lists the commands for various platforms. Choose and run the command corresponding to your hardware configuration.</p> Platform Docker Build Command Check Success Intel\u00ae integrated and Arc\u2122 GPUs <pre>cd benchmark-scriptsmake build-benchmarkmake build-igt</pre> Docker images command to show both <code>benchmark:dev</code> and <code>benchmark:igt</code> images Intel\u00ae Flex GPUs <pre>cd benchmark-scriptsmake build-benchmarkmake build-xpu</pre> Docker images command to show both <code>benchmark:dev</code> and <code>benchmark:xpu</code> images <p>Warning</p> <p>Build command may take a while, depending on your internet connection and machine specifications.</p> </li> <li> <p>Determine the appropriate parameters for</p> <ul> <li>Input source type</li> <li>Platform</li> <li>Pipeline Profile</li> </ul> </li> <li> <p>Choose a given pipeline profile, and run the benchmark for that pipeline profile. To see all available pipeline profiles, use <code>make list-profiles</code> command on the project base directory.</p> <pre><code># if you are in the benchmark-scripts directory then do a cd ..\n$ cd ..\n\n$ make list-profiles\n</code></pre> </li> <li> <p>Run the <code>benchmark.sh</code> shell script which can be found in the base/benchmark_scripts directory. The following example runs multiple pipelines benchmarking for the object detection pipelines.</p> <pre><code>cd ./benchmark_scripts\nPIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --pipelines &lt;number of pipelines&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt;\n</code></pre> <p>Note</p> <p>The <code>benchmark.sh</code> can either benchmark a specific number of pipelines or benchmark stream density based on the desired FPS.</p> </li> </ol>"},{"location":"OVMS/pipelinebenchmarking.html#input-source-type","title":"Input Source Type","text":"<p>The benchmark script can take either of the following video input sources:</p> <ul> <li> <p>Real Time Streaming Protocol (RTSP)</p> <pre><code>--inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre> <p>Note</p> <p>Using RTSP source with <code>benchmark.sh</code> will automatically run the camera simulator. The camera simulator will start an RTSP stream for each video file in the sample-media folder.</p> </li> <li> <p>USB Camera</p> <pre><code>--inputsrc /dev/video&lt;N&gt;, where N is 0 or an integer\n</code></pre> </li> <li> <p>Intel\u00ae RealSense\u2122 Camera</p> <pre><code>--inputsrc &lt;RealSense camera serial number&gt;\n</code></pre> <p>To know the serial number of the Intel\u00ae RealSense\u2122 Camera, refer to Get Serial Number of Intel\u00ae RealSense\u2122 Camera.</p> </li> <li> <p>Video File</p> <pre><code>--inputsrc file:my_video_file.mp4\n</code></pre> <p>Note</p> <p>Video files must be in the sample-media folder, so that the Docker container can access the files. You can provide your own video files or download a sample video file using the script download_sample_videos.sh.</p> </li> </ul>"},{"location":"OVMS/pipelinebenchmarking.html#platform","title":"Platform","text":"<ul> <li> <p>Intel\u00ae Core\u2122 Processor</p> <ul> <li><code>--platform core.x</code> if GPUs are available, then replace this parameter with targeted GPUs such as core (for all GPUs), core.0, core.1, and so on</li> <li><code>--platform core</code> will evenly distribute and utilize all available core GPUs</li> </ul> </li> <li> <p>Intel\u00ae Xeon\u00ae Scalable Processor</p> <ul> <li><code>--platform xeon</code> will use the Xeon CPU for the pipelines</li> </ul> </li> <li> <p>DGPU (Intel\u00ae Data Center GPU Flex 140,  Intel\u00ae Data Center GPU Flex 170, and Intel\u00ae Arc\u2122 Setup)</p> <ul> <li><code>--platform dgpu.x</code> replace this parameter with targeted GPUs such as dgpu (for all GPUs), dgpu.0, dgpu.1, and so on</li> <li><code>--platform dgpu</code> will evenly distribute and utilize all available dgpus</li> </ul> </li> </ul>"},{"location":"OVMS/pipelinebenchmarking.html#benchmark-specified-number-of-pipelines","title":"Benchmark Specified Number of Pipelines","text":"<p>The primary purpose of benchmarking with a specified number of pipelines is to discover the performance and system requirements for a given use case.</p> <p>Example</p> <p>Here is an example of running benchmarking object detection pipelines with specified number of pipelines:</p> <pre><code>PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --pipelines &lt;number of pipelines&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt;\n</code></pre> <p>where, the configurable input parameters include: </p> <ul> <li><code>--performance_mode</code> configures the scaling governor of the system. Supported modes are performance and powersave (default).</li> <li><code>--logdir</code> configures the benchmarking output directory</li> <li><code>--duration</code> configures the duration, in number of seconds, the benchmarking will run</li> <li><code>--init_duration</code> configures the duration, in number of seconds, to wait for system initialization before the benchmarking metrics or data collection begins</li> </ul> <p>Example</p> <p>You can run multiple pipeline benchmarking with different configurations before consolidating all pipeline output results.</p> <p>To get the consolidated pipeline results, run the following <code>make</code> command:</p> <pre><code>make consolidate ROOT_DIRECTORY=&lt;output dir&gt;\n</code></pre> <p>This command will consolidate the performance metrics that exist in the specified <code>ROOT_DIRECTORY</code>. </p> <p>Here is an example of consolidated output: </p> <p>Success</p> <p>Output of <code>Consolidate_multiple_run_of_metrics.py</code></p> <pre><code>,Metric,data\n0,Total Text count,0\n1,Total Barcode count,2\n2,Camera_1 FPS,15.0\n3,Camera_0 FPS,15.0\n4,CPU Utilization %,16.548\n5,Memory Utilization %,21.162\n6,Disk Read MB/s,0.0\n7,Disk Write MB/s,0.025\n8,S0 Memory Bandwidth Usage MB/s,1872.632\n9,S0 Power Draw W,27.502\n10,GPU_0 VDBOX0 Utilization %,0.0\n11,GPU_0 GPU Utilization %,17.282\n12,GPU_1 VDBOX0 Utilization %,0.0\n13,GPU_1 GPU Utilization %,0.0\n</code></pre>"},{"location":"OVMS/pipelinebenchmarking.html#benchmark-specified-profile","title":"Benchmark Specified Profile","text":"<p>There are several pipeline profiles that support different programming languages and different pipeline models. You may specify language choice and model input. Then you may prefix benchmark script run command with specific profile.</p> <p>Example</p> Stream density benchmark script in golangStream density benchmark script in python <pre><code>PIPELINE_PROFILE=\"grpc_go\" sudo -E ./benchmark.sh --stream_density 14.9 --logdir mytest/data --duration 60 --init_duration 20 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre> <pre><code>PIPELINE_PROFILE=\"grpc_python\" sudo -E ./benchmark.sh --stream_density 14.9 --logdir mytest/data --duration 60 --init_duration 60 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre> <p>Note</p> <p>If the prefix, <code>PIPELINE_PROFILE</code>, is not provided, then the default is \"grpc_python\".</p>"},{"location":"OVMS/pipelinebenchmarking.html#appendix-benchmark-helper-scripts","title":"Appendix: Benchmark Helper Scripts","text":"<ul> <li> <p><code>camera-simulator.sh</code>: This script starts the camera simulator. Create two folders named camera-simulator and sample-media. Place <code>camera-simulator.sh</code> in the camera-simulator folder. Manually copy the video files to the sample-media folder or run the <code>download_sample_videos.sh</code> script to download sample videos. The <code>camera-simulator.sh</code> script will start a simulator for each .mp4 video that it finds in the sample-media folder and will enumerate them as camera_0, camera_1, and so on. Make sure that the path to the <code>camera-simulator.sh</code> script is mentioned correctly in the <code>camera-simulator.sh</code> script.  </p> </li> <li> <p><code>stop_server.sh</code>: This script stops and removes all Docker containers closing the pipelines.</p> </li> </ul>"},{"location":"OVMS/pipelinerun.html","title":"Customized Run Pipeline","text":""},{"location":"OVMS/pipelinerun.html#overview","title":"Overview","text":"<p>When the pipeline is run, the <code>run.sh</code> script starts the service and performs inferencing on the selected input media. The output of running the pipeline provides the inference results for each frame based on the media source such as text, barcode, and so on, as well as the frames per second (FPS). Pipeline run provides many options in media type, system process platform type, and additional optional parameters. These options give you the opportunity to compare what system process platform is better for your need.</p>"},{"location":"OVMS/pipelinerun.html#start-pipeline","title":"Start Pipeline","text":"<p>You can run the pipeline script, <code>run.sh</code> with a given pipeline profile via the environment variable <code>PIPELINE_PROFILE</code>, and the following additional input parameters:</p> <ol> <li>Media type<ul> <li>Camera Simulator running using RTSP</li> <li>USB Camera using a supported output format</li> <li>Real Sense Camera using the serial number</li> <li>Video File</li> </ul> </li> <li>Platform<ul> <li>core</li> <li>dgpu.0</li> <li>dgpu.1</li> <li>xeon</li> </ul> </li> <li>Environment Variables</li> </ol> <p>Run the command based on your requirement. Once choices are selected for #1-3 above, to start the pipeline run, use the commands from the Examples section below.</p>"},{"location":"OVMS/pipelinerun.html#examples-using-different-input-source-types","title":"Examples using different input source types","text":"<p>In the following examples, environment variables are used to select the desired <code>PIPELINE_PROFILE</code> and <code>RENDER_MODE</code>. This table uses <code>run.sh</code> to run the object_detection pipeline profile:</p> Input source Type Input Source Parameter Command Simulated camera <code>rtsp://127.0.0.1:8554/camera_X</code> <code>PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc rtsp://127.0.0.1:8554/camera_1</code> RealSense camera <code>&lt;serial_number&gt; --realsense_enabled</code> <code>PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc  --realsense_enabled USB camera <code>/dev/video0</code> <code>PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc /dev/video0</code> Video file <code>file:my_video_file.mp4</code> <code>PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core|xeon|dgpu.x --inputsrc file:my_video_file.mp4</code> <p>Note</p> <p>The value of x in <code>dgpu.x</code> can be 0, 1, 2, and so on depending on the number of discrete GPUs in the system.</p>"},{"location":"OVMS/pipelinesetup.html","title":"Set up Pipeline","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone  https://github.com/intel-retail/automated-self-checkout.git &amp;&amp; cd ./automated-self-checkout\n</code></pre> </li> <li> <p>Build the profile launcher binary executable</p> <pre><code>make build-profile-launcher\n</code></pre> <p>Each profile is an unique pipeline use case.  We provide some profile examples, and the configuration examples of profiles are located here.  Go here to find out the detail descriptions for the configuration of profile used by profile launcher.</p> </li> <li> <p>Build the benchmark Docker images</p> <pre><code>cd benchmark-scripts\nmake build-all\n\ncd ..\n</code></pre> <p>Note</p> <p>A successfully built benchmark Docker images should contain the following Docker images from <code>docker images benchmark --format 'table{{.Repository}}\\t{{.Tag}}'</code> command:</p> <ul> <li>benchmark:dev</li> <li>benchmark:xpu</li> <li>benchmark:igt</li> </ul> <p>Note</p> <p>After successfully built benchmark Docker images, please remember to change the directory back to the project base directory from the current benchmark-scripts directory (i.e. <code>cd ..</code>) for the following steps.        </p> </li> <li> <p>Download the models manually (Optional)</p> <p>Note</p> <p>The model downloader script is automatically called as part of run.sh.</p> <pre><code>./download_models/getModels.sh\n</code></pre> <p>Warning</p> <p>Depending on your internet connection, this might take some time.</p> </li> <li> <p>(Optional) Download the video file manually. This video is used as the input source to give to the pipeline.</p> <p>Note</p> <p>The sample image downloader script is automatically called as part of run.sh. </p> <pre><code>./configs/opencv-ovms/scripts/image_download.sh\n</code></pre> <p>Warning</p> <p>Depending on your internet connection, this might take some time.</p> </li> <li> <p>(optional) Download the bit model manually </p> <p>a. Here is the command to build the container for bit model downloading:</p> <pre><code>docker build -f Dockerfile.bitModel -t bit_model_downloader:dev .\n</code></pre> <p>b. Here is the script to run the container that downloads the bit models:</p> <pre><code>docker run bit_model_downloader:dev\n</code></pre> </li> <li> <p>Build the reference design images. This table shows the commands for the OpenVINO (OVMS) model Server and profile-launcher build command:</p> Target Docker Build Command Check Success OVMS Server <pre>make build-ovms-server</pre> <code>docker images</code> command output contains Docker image openvino/model_server:2023.1-gpu OVMS Profile Launcher <pre>make build-profile-launcher</pre> <code>ls -al ./profile-launcher</code> command to show the binary executable <p>Note</p> <p>Build command may take a while, depending on your internet connection and machine specifications.</p> <p>Note</p> <p>If the build command succeeds, you will see all the built Docker images files as indicated in the Check Success column. If the build fails, check the console output for errors.</p> <p>Proxy</p> <p>If docker build system requires a proxy network, just set your proxy env standard way on your terminal as below and make build:</p> <pre><code>export HTTP_PROXY=\"http://your-proxy-url.com:port\"\nexport HTTPS_PROXY=\"https://your-proxy-url.com:port\"\nmake build-ovms-server\nmake build-profile-launcher\n</code></pre> </li> </ol>"},{"location":"OVMS/profileLauncherConfigs.html","title":"Profile Configuration","text":"<p>For the profile launcher, each profile has its own configuration for different pipelines.  The configuration of each profile is done through a yaml configuration file, configuration.yaml.  One example of configuration.yaml is shown here for classification profile:</p> <pre><code>OvmsSingleContainer: false\nOvmsServer:\n  ServerDockerScript: start_ovms_server.sh\n  ServerDockerImage: openvino/model_server:2023.1-gpu\n  ServerContainerName: ovms-server\n  ServerConfig: \"/models/config.json\"\n  StartupMessage: Starting OVMS server\n  InitWaitTime: 10s\n  EnvironmentVariableFiles:\n    - ovms_server.env\n  # StartUpPolicy:\n  # when there is an error on launching ovms server startup, choose one of these values for the behavior of profile-launcher: \n  #   remove-and-restart: it will remove the existing container with the same container name if any and then restart the container\n  #   exit: it will exit the profile-launcher and \n  #   ignore: it will ignore the error and continue (this is the default value if not given or none of the above)\n  StartUpPolicy: ignore    \nOvmsClient:\n  DockerLauncher:\n    Script: docker-launcher.sh\n    DockerImage: python-demo:dev\n    ContainerName: classification\n    Volumes:\n      - \"$RUN_PATH/results:/tmp/results\"\n      - ~/.Xauthority:/home/dlstreamer/.Xauthority\n      - /tmp/.X11-unix\n  PipelineScript: ./classification/python/entrypoint.sh\n  PipelineInputArgs: \"\" # space delimited like we run the script in command and take those input arguments\n  EnvironmentVariableFiles:\n    - classification.env\n</code></pre> <p>The description of each configuration element is explained below:</p> Configuration Element Description OvmsSingleContainer This boolean flag indicates whether this profile is running as a single OpenVino Model Server (OVMS) container or not, e.g. the C-API pipeline use case will use this as <code>true</code>. It can indicate the distributed architecture of OVMS client-server when this flag is false.  OvmsServer This is configuration section for OpenVino Model Server in the case of client-server architecture. OvmsServer/ServerDockerScript The infra-structure shell script to start an instance of OVMS server. OvmsServer/ServerDockerImage The Docker image tag name for OpenVino Model Server. OvmsServer/ServerContainerName The Docker container base name for OpenVino Model Server. OvmsServer/ServerConfig The model config.json file name path for OpenVino Model Server. OvmsServer/StartupMessage The starting message shown in the console or log when OpenVino Model Server instance is launched. OvmsServer/InitWaitTime The waiting time duration (like 5s, 5m, .. etc) after OpenVino Model Server is launched to allow some settling time before launching the pipeline from the client. OvmsServer/EnvironmentVariableFiles The list of environment variable files applied for starting OpenVino Model Server Docker instance. OvmsServer/StartUpPolicy This configuration controls the behavior of OpenVino Model Server Docker instance when there is error occurred during launching. Use one of these values: <code>remove-and-restart</code>: it will remove the existing container with the same container name if any and then restart the container  <code>exit</code>: it will exit the profile-launcher <code>ignore</code>: it will ignore the error and continue (this is the default value if not given or none of the above).  OvmsClient This is configuration section for the OVMS client running pipelines in the case of client-server architecture. The C-API pipeline use case should also use this section to configure. OvmsClient/DockerLauncher This is configuration section for the generic Docker launcher to run pipelines for a given profile. OvmsClient/DockerLauncher/Script The generic Docker launcher script file name. OvmsClient/DockerLauncher/DockerImage The Docker image tag name for the pipeline profile. OvmsClient/DockerLauncher/ContainerName The Docker container base name for the running pipeline profile. OvmsClient/DockerLauncher/Volumes The Docker container volume mounts for the running pipeline profile. OvmsClient/PipelineScript The file name path for the pipeline profile to launch. The file path here is in the perspective of the running container. i.e. the path inside the running container. OvmsClient/PipelineInputArgs Any input arguments or parameters for the above pipeline script to take. Like any command line argument, they are space-delimited if multiple arguments. OvmsClient/EnvironmentVariableFiles The list of environment variable files applied for the running pipeline profile Docker instance."},{"location":"OVMS/quick_pipelinerun.html","title":"Quick Start Guide to Run Pipeline","text":""},{"location":"OVMS/quick_pipelinerun.html#prerequisites","title":"Prerequisites","text":"<p>Before running, set up the pipeline.</p>"},{"location":"OVMS/quick_pipelinerun.html#running-ovms-with-the-camera-simulator","title":"Running OVMS with the camera simulator","text":""},{"location":"OVMS/quick_pipelinerun.html#start-the-camera-simulator","title":"Start the Camera Simulator","text":"<ol> <li> <p>Download the video files to the sample-media directory:     <pre><code>cd benchmark-scripts;\n./download_sample_videos.sh;\ncd ..;\n</code></pre></p> <p>Example - Specify Resolution and Framerate</p> <p>This example downloads a sample video for 1080p@15fps. <pre><code>cd benchmark-scripts;\n./download_sample_videos.sh 1920 1080 15;\ncd ..;\n</code></pre></p> <p>Note</p> <p>Only AVC encoded files are supported.</p> </li> <li> <p>After the video files are downloaded to the sample-media folder, start the camera simulator:     <pre><code>make run-camera-simulator\n</code></pre></p> </li> <li> <p>Wait for few seconds, and then check if the camera-simulator containers are running:     <pre><code>docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}'\n</code></pre></p> <p>Success</p> <p>Your output is as follows:</p> IMAGE STATUS NAMES openvino/ubuntu20_data_runtime:2021.4.2 Up 11 seconds camera-simulator0 aler9/rtsp-simple-server Up 13 seconds camera-simulator <p>Note</p> <p>There could be multiple containers with the image \"openvino/ubuntu20_data_runtime:2021.4.2\", depending on the number of sample-media video files provided.</p> <p>Failure</p> <p>If all the Docker* containers are not visible, then review the console output for errors. Sometimes dependencies fail to resolve. Address obvious issues and retry.</p> </li> </ol>"},{"location":"OVMS/quick_pipelinerun.html#run-instance-segmentation","title":"Run Instance Segmentation","text":"<p>There are several pipeline profiles to chose from. Use the <code>make list-profiles</code> to see the different pipeline options. In this example, the <code>instance_segmentation</code> pipeline profile will be used. </p> <ol> <li> <p>Use the following command to run instance segmentation using OVMS on core.</p> <pre><code>PIPELINE_PROFILE=\"instance_segmentation\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre> </li> <li> <p>Check the status of the pipeline.</p> <pre><code>docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}' -a\n</code></pre> <p>Success</p> <p>Here is a sample output:</p> IMAGE STATUS NAMES openvino/model_server-gpu:latest Up 59 seconds ovms-server0 <p>Failure</p> <p>If you do not see above Docker container(s), review the console output for errors. Sometimes dependencies fail to resolve and must be run again. Address obvious issues and try again repeating the above steps. Here are couple debugging tips:</p> <ol> <li> <p>check the docker logs using following command to see if there is an issue with the container</p> <pre><code>docker logs &lt;containerName&gt;\n</code></pre> </li> <li> <p>check ovms log in automated-self-checkout/results/r0.jsonl</p> </li> </ol> </li> <li> <p>Check the output in the <code>results</code> directory.</p> <p>Example - results/r0.jsonl sample</p> <p>The output in results/r0.jsonl file lists average processing time in milliseconds and average number of frames per second. This file is intended for scripts to parse.  <pre><code>Processing time: 53.17 ms; fps: 18.81\nProcessing time: 47.98 ms; fps: 20.84\nProcessing time: 48.35 ms; fps: 20.68\nProcessing time: 46.88 ms; fps: 21.33\nProcessing time: 47.56 ms; fps: 21.03\nProcessing time: 49.66 ms; fps: 20.14\nProcessing time: 52.49 ms; fps: 19.05\nProcessing time: 52.27 ms; fps: 19.13\nProcessing time: 50.86 ms; fps: 19.66\nProcessing time: 58.19 ms; fps: 17.18\nProcessing time: 58.28 ms; fps: 17.16\nProcessing time: 52.17 ms; fps: 19.17\nProcessing time: 50.89 ms; fps: 19.65\nProcessing time: 49.58 ms; fps: 20.17\nProcessing time: 51.14 ms; fps: 19.55\n</code></pre></p> <p>Example - results/pipeline0.log sample</p> <p>The output in results/pipeline0.log lists average number of frames per second. Below is a snap shot of the output: <pre><code>18.81\n20.84\n20.68\n21.33\n21.03\n20.14\n19.05\n19.13\n19.66\n17.18\n17.16\n19.17\n19.65\n20.17\n19.55\n</code></pre></p> <p>Note</p> <p>The automated-self-checkout/results/ directory is volume mounted to the pipeline container.</p> </li> </ol>"},{"location":"OVMS/quick_pipelinerun.html#stop-running-the-pipelines","title":"Stop running the pipelines","text":"<ol> <li>To stop the instance segmentation container and clean up, run      <pre><code>make clean-all\n</code></pre></li> </ol>"},{"location":"OVMS/quick_stream_density.html","title":"Quick Start Guide to Run Pipeline Stream Density","text":"<p>In this section, we show the steps to run the stream density for a chosen pipeline profile.  By definition, the objective of the stream density is to bench-mark the maximum number of multiple running pipelines at the same time while still maintaining the goal-setting target frames-per-second (FPS).</p>"},{"location":"OVMS/quick_stream_density.html#prerequisites","title":"Prerequisites","text":"<p>Before running, set up the pipeline if not already done.</p>"},{"location":"OVMS/quick_stream_density.html#stop-all-other-running-pipelines","title":"Stop All Other Running Pipelines","text":"<p>To make sure we have a good stream density benchmarking, it is recommended to stop all other running pipelines before running the stream density. To stop all running pipelines and clean up, run     <pre><code>make clean-all\n</code></pre></p>"},{"location":"OVMS/quick_stream_density.html#build-benchmark-docker-images","title":"Build Benchmark Docker Images","text":"<p>For running stream density, the benchmark scripts are utilized.  To set up the benchmarking, we need to build the benchmark Docker images first.</p> <ol> <li> <p>Build the benchmark Docker* images     Benchmark scripts are containerized inside Docker. These scripts are dependent on the platform.</p> Intel\u00ae integrated and Arc\u2122 GPUsIntel\u00ae Flex GPUs <pre><code>cd ./benchmark-scripts\nmake build-benchmark\nmake build-igt\n</code></pre> <p>Success</p> <p>Run <code>docker images</code> to ensure that both the <code>benchmark:dev</code> and <code>benchmark:igt</code> images were built.</p> <pre><code>cd ./benchmark-scripts\nmake build-benchmark\nmake build-xpu\n</code></pre> <p>Success</p> <p>Run <code>docker images</code> to ensure that both the <code>benchmark:dev</code> and <code>benchmark:xpu</code> images were built.</p> </li> </ol> <p>Warning</p> <p>Build command may take a while, depending on your internet connection and machine specifications.</p>"},{"location":"OVMS/quick_stream_density.html#start-the-camera-simulator","title":"Start the Camera Simulator","text":"<p>We will use the camera simulator as the input source to show the stream density. Please refer to the section of Start the Camera Simulator in Quick Start Guide to Run Pipeline on how to start the camera simulator.</p>"},{"location":"OVMS/quick_stream_density.html#run-objection-detection-pipeline-stream-density","title":"Run Objection Detection Pipeline Stream Density","text":"<p>There are several pipeline profiles to choose from for running pipeline stream density. Use the <code>make list-profiles</code> to see the different pipeline options. In this example, the <code>object_detection</code> pipeline profile will be used for running stream density.</p> <ol> <li> <p>To run the stream density, the benchmark shell script, <code>benchmark.sh</code>, is used. The script is in the &lt;project_base_dir&gt;/benchmark-scripts directory.  Use the following command to run objection detection pipeline profile using OVMS on core.</p> <pre><code>cd ./benchmark-scripts\nPIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --stream_density 15.0 --logdir object_detection/data --duration 120 --init_duration 40 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1\n</code></pre> <p>Note</p> <p>Description of some key benchmarking input parameters is given as below:</p> Parameter Name Example Value Description --stream_density 15.0 The value 15.0 after the --stream_density is the target FPS that we want to achieve for running maximum number of object detection pipelines while the averaged of all pipelines from the output FPS still maintaining that target FPS value. --logdir object_detection/data the output directory of benchmarking resource details --duration 120 the time duration, in number of seconds, the benchmarking will run --init_duration 40 the time duration, in number of seconds, to wait for system initialization before the benchmarking metrics or data collection begins <p>Note</p> <p>For stream density run, it is recommended to turn off the display to conserve the system resources hence setting <code>RENDER_MODE=0</code></p> <p>Note</p> <p>This takes a while for the whole stream density benchmarking process depending on your system resources like CPU, memory, ... etc.</p> <p>Note</p> <p>The benchmark.sh script automatically cleans all running Docker containers after it is done.</p> <p>If the hardware supports, then one can also run the benchmarking on different devices like CPU or GPU.  This can be done through the environment variable <code>DEVICE</code>.  The following is an example to run the object_detection profile using GPU:</p> <pre><code>PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 DEVICE=\"GPU\" sudo -E ./benchmark.sh --stream_density 14.95 --logdir object_detection/data --duration 120 --init_duration 40 --platform dgpu.0 --inputsrc rtsp://127.0.0.1:8554/camera_1\n</code></pre> <p>Note</p> <p>The performance of running object detection benchmarking should be better while running on GPU using model precision FP32.  If supported, then you can change the model precision by going to the folder <code>configs/opencv-ovms/models/2022</code> from the root of project folder and editing the <code>base_path</code> for that particular model in the <code>config_template.json</code> file.  For example, you can change the the base_path of <code>FP32</code> to <code>FP16</code> assuming the precision <code>FP16</code> of the model is available: </p> <pre><code>    ...\n    \"config\": {\n    \"name\": \"ssd_mobilenet_v1_coco\",\n    \"base_path\": \"/models/ssd_mobilenet_v1_coco/FP32\",\n    ...\n    }\n</code></pre> <p>The directory structure of models with both precisions should look like the followings:</p> <pre><code>ssd_mobilenet_v1_coco\n\u251c\u2500\u2500 FP32\n|   \u2514\u2500\u2500 1\n|       \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin\n|       \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml\n\u251c\u2500\u2500 FP16\n|   \u2514\u2500\u2500 1\n|       \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin\n|       \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml\n</code></pre> </li> <li> <p>Check the output in the base <code>results</code> directory.</p> <p>After the stream density is done, the results of stream density can be seen on the base directory of the <code>results</code> directory:</p> <pre><code>cat ../results/stream_density.log\n</code></pre> <p>Example - results/stream_density.log sample</p> <p>The output in results/stream_density.log file gives the detailed information of stream density results:  <pre><code>   ......\n   FPS for pipeline0: 15.1225\n   FPS for pipeline1: 15.19\n   FPS for pipeline2: 15.18\n   Total FPS throughput: 45.4925\n   Total FPS per stream: 15.1642\n   Max stream density achieved for target FPS 15.0 is 3\n   Finished stream density benchmarking\n   stream_density done!\n</code></pre></p> </li> </ol>"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html","title":"Run Object Detection Pipeline with New Model","text":"<p>OpenVINO Model Server has many ways to run inferencing pipeline: TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). For running object detection pipeline, it is based on KServe gRPC API method, default model used is ssd_mobilenet_v1_coco. You can use different model to run object detection. Here are the steps:</p> <ol> <li>Add new section to config file for model server</li> <li>Download new model</li> <li>Update environment variables of detection pipeline for new model</li> <li>Build and Run</li> </ol>"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#add-new-section-to-config-file-for-model-server","title":"Add New Section to Config File for Model Server","text":"<p>Here is the config file location: <code>configs/opencv-ovms/models/2022/config_template.json</code>, edit the file and append the following configuration section template <pre><code>,\n{\n      \"config\": {\n        \"name\": \"ssd_mobilenet_v1_coco\",\n        \"base_path\": \"/models/ssd_mobilenet_v1_coco/FP32\",\n        \"nireq\": 1,\n        \"batch_size\": \"1\",\n        \"plugin_config\": {\n          \"PERFORMANCE_HINT\": \"LATENCY\"\n        },\n        \"target_device\": \"{target_device}\"\n      },\n      \"latest\": {\n        \"num_versions\": 1\n      }\n    }\n</code></pre></p> <p>Note</p> <p>Please leave <code>target_device</code> value as it is, as the value <code>{target_device}</code> will be recognized and replaced by script run.</p> <p>You can find the parameter description in the ovms docs.</p>"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#download-new-model","title":"Download New Model","text":"<p>The pipeline run script automatically download the model files if it is part of open model zoo supported list; otherwise, please add your model files manually to <code>configs/opencv-ovms/models/2022/</code>. When you add your model manually, make sure to follow the model file structure as //1/modelfiles, for example: <pre><code>ssd_mobilenet_v1_coco\n\u251c\u2500\u2500 FP32\n\u00a0\u00a0 \u2514\u2500\u2500 1\n\u00a0\u00a0     \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin\n\u00a0\u00a0     \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml\n</code></pre>"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#update-environment-variables","title":"Update Environment Variables","text":"<p>You can update the object detection environment variables in file: <code>configs/opencv-ovms/envs/object_detection.env</code>, here is default value and explanation for each environment variable:</p> EV Name Default Value Description DETECTION_MODEL_NAME ssd_mobilenet_v1_coco model name for object detection DETECTION_LABEL_FILE coco_91cl_bkgr.txt label file name to use on object detection for model DETECTION_ARCHITECTURE_TYPE ssd architecture type for object detection model DETECTION_OUTPUT_RESOLUTION 1920x1080 output resolution for object detection result DETECTION_THRESHOLD 0.50 threshold for object detection in floating point that needs to be between 0.0 to 1.0 MQTT enable MQTT notification of result, value: empty RENDER_MODE 1 display the input source video stream with the inferencing results, value: 0"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#build-and-run-pipeline","title":"Build and Run Pipeline","text":"<ol> <li> <p>Build the python app and profile-launcher:</p> <pre><code> make build-python-apps\n</code></pre> </li> <li> <p>Download sample video files:</p> <pre><code>cd benchmark-scripts/ &amp;&amp; ./download_sample_videos.sh &amp;&amp; cd ..\n</code></pre> </li> <li> <p>Start simulator camera if not started:</p> <pre><code>make run-camera-simulator\n</code></pre> </li> <li> <p>(Optional) Run MQTT broker:</p> <pre><code>docker run --network host --rm -d -it -p 1883:1883 -p 9001:9001 eclipse-mosquitto\n</code></pre> </li> <li> <p>Start the object detection pipeline:</p> <pre><code>PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 MQTT=127.0.0.1:1883 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre> <p>Note</p> <p>Remove the MQTT environment variable if not using it.</p> </li> <li> <p>If using MQTT, use the same container name as the MQTT topic to subscribe to the inference metadata. Use the command <code>docker ps</code> to know the container name.</p> </li> <li> <p>Use one of the following commands for stopping and cleaning up:</p> <ol> <li> <p>To stop and clean up the client side containers, run:</p> <pre><code>make clean-profile-launcher\n</code></pre> </li> <li> <p>To stop and clean up everything, run:</p> <pre><code>make clean-all\n</code></pre> </li> </ol> </li> </ol>"},{"location":"OVMS/stop_pipeline_run.html","title":"Stop pipeline run","text":"<p>You can call <code>make clean-ovms</code> to stop the pipeline and all running containers for OVMS, hence the results directory log files will stop growing. Below is the table of make commands you can call to clean things up per your needs:</p> Clean-Up Options Command clean instance-segmentation container if any <pre>make clean-segmentation</pre> clean grpc-go dev container if any <pre>make clean-grpc-go</pre> clean all related containers launched by profile-launcher if any <pre>make clean-profile-launcher</pre> clean ovms-server container <pre>make clean-ovms-server</pre> clean ovms-server and all containers launched by profile-launcher <pre>make clean-ovms</pre> clean results/ folder <pre>make clean-results</pre> clean all downloaded models <pre>make clean-models</pre> stops pipelines and cleans all containers, simulator, results, telemetry and webcam <pre>make clean-all</pre>"},{"location":"OVMS/supportingDifferentLanguage.html","title":"Supporting Different Languages","text":"<p>For running OVMS as inferencing engine through grpc, these are the supported languages:</p> <ol> <li>python</li> <li>golang</li> </ol>"},{"location":"OVMS/supportingDifferentLanguage.html#sample-commands-using-the-camera-simulator","title":"Sample Commands using the Camera Simulator","text":"Input source Type Command Python <code>PIPELINE_PROFILE=\"grpc_python\" sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1</code> Golang <code>PIPELINE_PROFILE=\"grpc_go\" sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1</code> <p>Note</p> <p>Above example scripts are based on camera simulator for rtsp input source, before running them, please run the camera simulator. If you used a different input source, fill in the appropriate value for <code>--inputsrc</code>.</p>"},{"location":"OVMS/supportingDifferentModel.html","title":"Supporting Different Models","text":"<p>For running OVMS as inferencing engine through grpc, we are supporting different models for your need. </p>"},{"location":"OVMS/supportingDifferentModel.html#models-supported-in-python","title":"Models Supported In Python","text":"<p>Here is the list of inferencing models we are currently supporting in python:</p> <ol> <li>instance-segmentation-security-1040</li> <li>bit_64</li> </ol> <p>You can switch between them by editing the configuration file <code>configs/opencv-ovms/cmd_client/res/grpc_python/configuration.yaml</code>, uncomment <code># PipelineInputArgs: \"--model_name instance-segmentation-security-1040\"</code> for supporting instance-segmentation-security-1040 and comment out rest; or you can uncomment <code># PipelineInputArgs: \"--model_name bit_64\"</code> for supporting bit_64 and comment out rest.</p> <p>Here is the configuration.yaml content, default to use <code>instance-segmentation-security-1040</code> model <pre><code>OvmsClient:\n  PipelineScript: run_grpc_python.sh\n  PipelineInputArgs: \"--model_name instance-segmentation-security-1040\" # space delimited like we run the script in command and take those input arguments\n  # PipelineInputArgs: \"--model_name bit_64\" # space delimited like we run the script in command and take those input arguments\n  # PipelineInputArgs: \"--model_name yolov5s\" # space delimited like we run the script in command and take those input arguments\n</code></pre></p>"},{"location":"OVMS/supportingDifferentModel.html#download-models","title":"Download Models","text":"<p>You can download models by editing <code>download_models/models.lst</code> file, you can add new models(from https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/object_detection_demo/python/models.lst) to it or uncomment from existing list in this file, saved the file once editing is done. Then you can download the list using following steps:</p> <ol> <li><code>cd download_models</code></li> <li><code>make build</code></li> <li><code>make run</code></li> </ol> <p>after above steps, the downloaded models can be found in <code>configs/opencv-ovms/models/2022</code> directory.</p> <p>Note</p> <p>Model files in <code>configs/opencv-ovms/models/2022</code> directory will be replaced with new downloads if previously existed.</p>"},{"location":"dev-tools/documentation.html","title":"Documentation Tools","text":"<ol> <li> <p>Project uses mkdocs and mkdocs-material to maintain the documentation on github pages.</p> </li> <li> <p>Github page deployments are automated via github-actions - page-build-deployment and Update GH Pages,  which get executed everytime a branch with documentation changes is merged into the main branch</p> </li> <li> <p>Documentation versioning is implemented using the mike tool.</p> </li> </ol>"},{"location":"dev-tools/documentation.html#mike-tool-usage","title":"Mike Tool Usage","text":"<ol> <li> <p>Make sure page-build-deployment and Update GH Pages are working</p> <p>Note</p> <p>Mike tool works with Github Pages(gh-pages), versioning changes will appear only after they are deployed to gh-pages via github actions. To run them locally, use the following command to view documentation at http://localhost:8000/ <pre><code>mike serve\n</code></pre></p> </li> <li> <p>Install mike tool on local development environment</p> <pre><code>pip install mike\n</code></pre> </li> <li> <p>Make the documentation changes and check in the code.</p> </li> <li> <p>Alias for documentation default version is set as latest. If required, this can be changed -</p> <pre><code>mike set-default --push latest\n</code></pre> </li> <li> <p>Publish a new version of project documentation by choosing a version identifier and update the alias set as the default version with -</p> <pre><code>mike deploy --push --update-aliases 0.1 latest\n</code></pre> </li> <li> <p>Every version will be deployed as a subdirectory of site_url set in mkdocs.yaml. Example the documentation will be published to URLs such as:</p> <pre><code>https://intel-retail.github.io/automated-self-checkout/0.1/\nhttps://intel-retail.github.io/automated-self-checkout/0.2/\n</code></pre> </li> <li> <p>After PR is merged into the main branch, github actions will deploy the gh-pages and version dropdown on the documentation page will populate the new version as shown below</p> <p></p> </li> <li> <p>To delete versions, use the following commands -</p> <pre><code>mike delete [version identifier]...\n</code></pre> </li> <li> <p>To list versions, use the following commands -</p> <pre><code>mike list\n</code></pre> </li> </ol>"},{"location":"dev-tools/environment_variables.html","title":"Environment Variables (EVs)","text":"<p>The table below lists the environment variables (EVs) that can be used as inputs for the container running the inferencing pipeline.</p> GST profile EVsCommon EVs <p>This list of EVs specifically supports the GST profile DLStreamer workloads.</p> Variable Description Values <code>AGGREGATE</code> aggregate branches of the gstreamer pipeline, if any, at the end of the pipeline \"\", \"gvametaaggregate name=aggregate\", \"aggregate branch. ! queue\" <code>BARCODE_RECLASSIFY_INTERVAL</code> time interval in seconds for barcode classification Ex: 5 <code>CLASSIFICATION_OPTIONS</code> extra classification pipeline instruction parameters \"\", \"reclassify-interval=1 batch-size=1 nireq=4 gpu-throughput-streams=4\" <code>CPU_SW_DECODER</code> force to use software decoder for gst-launch decoding video frames in CPU \"force-sw-decoders=1\" <code>DECODE</code> decoding element instructions for gst-launch to use Ex: \"decode bin force-sw-decoders=1\" <code>DETECTION_OPTIONS</code> extra object detection pipeline instruction parameters \"\", \"gpu-throughput-streams=4 nireq=4 batch-size=1\" <code>GST_DEBUG</code> for running pipeline in gst debugging mode 0, 1 <code>GST_PIPELINE_LAUNCH</code> for launching gst pipeline script file path and name Ex: \"/home/pipeline-server/framework-pipelines/yolov5_pipeline/yolov5s.sh\" <code>LOG_LEVEL</code> log level to be set when running gst pipeline ERROR, INFO, WARNING, and more <code>OCR_RECLASSIFY_INTERVAL</code> time interval in seconds for OCR classification Ex: 5 <code>PARALLEL_PIPELINE</code> run pipeline in parallel using the tee branch \"\", \"! tee name=branch ! queue\" <code>PARALLEL_AGGRAGATE</code> aggregate parallel pipeline results together, paired use with PARALLEL_PIPELINE \"\", \"! gvametaaggregate name=aggregate ! gvametaconvert name=metaconvert add-empty-results=true ! gvametapublish name=destination file-format=2 file-path=/tmp/results/r$cid_count.jsonl ! fpsdisplaysink video-sink=fakesink sync=true --verbose branch. ! queue !\" <code>VA_SURFACE</code> use video analytics surface from the shared memory if applicable \"\", \"! \"video/x-raw(memory <p>This list of EVs is common for all profiles.</p> Variable Description Values <code>AUTO_SCALE_FLEX_140</code> allow workload to manage autoscaling 1, 0 <code>CPU_ONLY</code> for overriding inference to be performed on CPU only 1, 0 <code>DEVICE</code> for setting device to use for pipeline run \"GPU\", \"CPU\", \"AUTO\", \"MULTI:GPU,CPU\" <code>LOW_POWER</code> for running pipelines using GPUs only 1, 0 <code>OCR_DEVICE</code> optical character recognition device \"CPU\", \"GPU\" <code>PRE_PROCESS</code> pre process command to add for inferencing \"pre-process-backend=vaapi-surface-sharing\", \"pre-process-backend=vaapi-surface-sharing pre-process-config=VAAPI_FAST_SCALE_LOAD_FACTOR=1\" <code>PIPELINE_PROFILE</code> for choosing OVMS workload's pipeline profile to run use <code>make list-profiles</code> to see Values <code>RENDER_MODE</code> for displaying pipeline and overlay CV metadata 1, 0 <code>STREAM_DENSITY_MODE</code> for starting pipeline stream density testing 1, 0 <code>STREAM_DENSITY_FPS</code> for setting stream density target fps value Ex: 15.0 <code>STREAM_DENSITY_INCREMENTS</code> for setting incrementing number of pipelines for running stream density Ex: 1"},{"location":"dev-tools/environment_variables.html#applying-ev-to-run-pipeline","title":"Applying EV to Run Pipeline","text":"<p>EV can be applied in two ways:</p> <pre><code>1. as parameter input to run.sh script\n2. in the env files\n</code></pre> <p>The input parameter will override the one in the env files if both are used.</p>"},{"location":"dev-tools/environment_variables.html#ev-as-input-parameter","title":"EV as input parameter","text":"<p>Example</p> <pre><code>PIPELINE_PROFILE=\"object_detection\" CPU_ONLY=1 RENDER_MODE=0 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre> <p>Note</p> <p>Those EVs in front of run.sh like <code>CPU_ONLY</code>, <code>RENDER_MODE</code> are applied to this run only and they are also known as command line environment overrides, or environment overrides.  They will override the default values in env files if any.</p>"},{"location":"dev-tools/environment_variables.html#editing-the-env-files","title":"Editing the Env Files","text":"<p>EV can be configured for advanced user in <code>configs/opencv-ovms/envs/</code>.  As an example for gst pipeline profile, there are two Env files can be configured:</p> <pre><code>    - `yolov5-cpu.env` file for running pipeline in core system\n    - `yolov5-gpu.env` file for running pipeline in gpu or multi\n</code></pre> <p>These two files currently hold the default values.</p>"},{"location":"dev-tools/overview.html","title":"Developer Overview","text":"<p>Developers can learn more about specific tools available to assist in running pipelines, benchmarking and documentation.</p>"},{"location":"dev-tools/references.html","title":"References","text":""},{"location":"dev-tools/references.html#libraries","title":"Libraries","text":"Library Link Intel\u00ae Deep Learning Streamer (Intel\u00ae DL Streamer) https://github.com/dlstreamer/dlstreamer gstreamer plugin for Intel\u00ae RealSense\u2122 Cameras https://github.com/G2020sudo/realsense-gstreamer Libraries for Intel\u00ae RealSense\u2122 Cameras https://github.com/gwen2018/librealsense"},{"location":"dev-tools/references.html#components","title":"Components","text":"Component Link gstreamer https://gstreamer.freedesktop.org/ Intel\u00ae RealSense\u2122 Technology https://www.intel.com/content/www/us/en/architecture-and-technology/realsense-overview.html"},{"location":"dev-tools/run_camera_simulator.html","title":"Camera Simulator","text":""},{"location":"dev-tools/run_camera_simulator.html#overview","title":"Overview","text":"<p>If you do not have a camera device plugged into the system, run the camera simulator to view the pipeline analytic results based on a sample video file to mimic real time camera video. You can also use the camera simulator to infinitely loop through a video file for consistent benchmarking. For example, if you want to validate whether the performance is the same for 6 hours, 12 hours, and 24 hours, looping the same video should produce the same results regardless of the duration.</p>"},{"location":"dev-tools/run_camera_simulator.html#run-the-camera-simulator","title":"Run the Camera Simulator","text":"<p>Follow the steps below to run the camera simulator:</p> <ol> <li> <p>Download the video files to the sample-media directory:     <pre><code>cd benchmark-scripts;\n./download_sample_videos.sh;\ncd ..;\n</code></pre></p> <p>Example</p> <p>This example downloads a sample video for 1080p@15fps. <pre><code>   cd benchmark-scripts;\n   ./download_sample_videos.sh 1920 1080 15;\n   cd ..;\n</code></pre></p> <p>Note</p> <p>Only AVC encoded files are supported.</p> </li> <li> <p>After the video files are downloaded to the sample-media folder, start the camera simulator:     <pre><code>make run-camera-simulator\n</code></pre></p> </li> <li> <p>Wait for few seconds, and then check if the camera-simulator containers are running:     <pre><code>docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}'\n</code></pre></p> <p>Success</p> <p>Your output is as follows:</p> IMAGE STATUS NAMES openvino/ubuntu20_data_runtime:2021.4.2 Up 11 seconds camera-simulator0 aler9/rtsp-simple-server Up 13 seconds camera-simulator <p>Note</p> <p>There could be multiple containers with the image \"openvino/ubuntu20_data_runtime:2021.4.2\", depending on the number of sample-media video files provided.</p> <p>Failure</p> <p>If all the Docker* containers are not visible, then review the console output for errors. Sometimes dependencies fail to resolve. Address obvious issues and retry.</p> </li> </ol>"},{"location":"dev-tools/run_camera_simulator.html#stopping-the-camera-simulator","title":"Stopping the Camera Simulator","text":"<ol> <li> <p>The camera simulator may be stopped and cleaned up by running </p> <pre><code>make clean-simulator\n</code></pre> </li> </ol>"},{"location":"dev-tools/telemetry/setup.html","title":"Setup Telemetry","text":"<ol> <li> <p>Build Telegraf docker image</p> <pre><code>make build-telegraf\n</code></pre> </li> <li> <p>Run InfluxDB and Telegraf. Set password for InfluxDB as env variable in command line:</p> <p>Note</p> <p>Password must be at least 8 characters in length.</p> <pre><code>make INFLUXPASS=yourpass run-telegraf\n</code></pre> </li> <li> <p>Start the dashboard by opening a browser to http://127.0.0.1:8086 and login using username: telegraf and the password you set previously.</p> </li> <li> <p>Click on \"Build a Dashboard\", then \"Import dashboard\" and select the file intel_core_and_igpu_telemetry.json under telegraf folder.</p> <p></p> </li> <li> <p>Run the dashboard</p> <p></p> </li> </ol>"},{"location":"release-notes/v1-0-1.html","title":"1.0.1","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.0.0 is the first major release. This release includes all items required to run the vision checkout pipeline and benchmarking. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-0-1.html#new-features","title":"New Features","text":"Title Description Pipeline scripts Scripts that run the GStreamer-based vision checkout pipeline Benchmark scripts Scripts that start pipelines and system metrics based on parameters Docker* images Dockerized images for the pipeline and benchmark tools for code portability Set up Documentation Markdown files that include setup steps for initial use Unit tests Basic unit tests scripts for smoke testing Camera simulator Camera simulator script to simulate an RTSP stream using a media file Media downloader script Script to assist with downloading sample media for the camera simulator Model downloader script Script to assist with downloading the model files used for the pipelines"},{"location":"release-notes/v1-0-1.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description Link None Initial Release"},{"location":"release-notes/v1-0-1.html#known-issues","title":"Known Issues","text":"Issue Number Description Link 15 Pipeline core run on some HW is not producing inference results https://github.com/intel-retail/vision-self-checkout/issues/15 29 Unable to modify batch size from run script https://github.com/intel-retail/vision-self-checkout/issues/29"},{"location":"release-notes/v1-5-0.html","title":"1.5.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.5.0 is the second main release. This release includes bug fixes, feature enhancements, dockerization of the benchmarking tools, and OpenVINO Model Server support. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-5-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server OpenVINO Model Server support OpenVINO Model Server Pipelines Object detection pipelines using OpenVINO Model Server Benchmark scripts Dockerization Benchmark tools have been moved to Docker containers for more flexible deployment Github Build actions Code linting and security scans for pull requests"},{"location":"release-notes/v1-5-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 41 Pipeline failure log 42 Create makefile docker commands 51 Optimized density script to reduce run time on high powered systems 55 Make performance / powersave mode configurable 57 Add debug option for docker-run.sh 58 Doc update with makefile 61 rename vision self checkout to automated self checkout 65 Update documentation to include OVMS pipelines 66 Add model download top level script 67 [Tech Debt] Make --workload work in any option/argument position when run benchmark.sh 75 docker-run.sh with wrong message when no --workload option is provided 77 XPU Manager not running on multiple GPUs 85 Fix ShellCheck issues in scripts 88 Incorrect instructions for building IGT in pipelinebenchmarking.md 91 format avc mp4 tag logic is inverted 96 For ovms workload getModels.sh not working when it is called by docker-run.sh from project base directory 99 Clean up some checked in dlstreamer models 100 Add cleaning ovms containers to makefile 105 benchmark pcm directory incorrect 109 igt path pointing to the incorrect directory causing the igt log to not be written 112 make CPU as default device for ovms pipeline 115 add dockerfile.bitModel to download bit models 119 pipelinesetup doc has incorrect link to models.list.yml 124 add ovms sample image download into run script 129 Update License to Apache 2.0 131 update mkdoc to navigate to OVMS doc 142 make build-ovms-server failed for 2nd time or later after removed the Docker image for rebuild"},{"location":"release-notes/v1-5-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"release-notes/v2-0-0.html","title":"2.0.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 2.0.0 is the next major release. This release includes bug fixes, feature enhancements, expansion of the OpenVINO Model Server use cases, implementation of gRPC and C API OVMS pipelines. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v2-0-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server gRPC OpenVINO Model Server support OpenVINO Model Server C API OpenVINO Model Server C API example Github Integration Test Action Nightly integration test action Docker Compose Pipeline Docker Compose version of the pipeline"},{"location":"release-notes/v2-0-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 2.0 Issues Closed Github issues closed in the 2.0 release"},{"location":"release-notes/v2-0-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"}]}