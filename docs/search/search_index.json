{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Intel\u00ae Automated Self-Checkout Reference Package","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>As Computer Vision becomes more and more mainstream, especially for industrial &amp; retail use cases, development and deployment of these solutions becomes more challenging. Vision workloads are large and complex and need to go through many stages as shown in the figure below.</p> <p></p> <p>Automated self-checkout solutions are complex, and retailers, independent software vendors (ISVs), and system integrators (SIs) require a good understanding of hardware and software, the costs involved in setting up and scaling the system, and the configuration that best suits their needs. Vision workloads are significantly larger and require systems to be architected, built, and deployed with several considerations. Hence, a set of ingredients needed to create an automated self-checkout solution is necessary. More details are available on the Intel Developer Focused Webpage and on this LinkedIn Blog</p> <p>The Intel\u00ae Automated Self-Checkout Reference Package provides critical components required to build and deploy a self-checkout use case using Intel\u00ae hardware, software, and other open-source software. This reference implementation provides a pre-configured automated self-checkout pipeline that is optimized for Intel\u00ae hardware.  The reference solution also includes a set of benchmarking tools to evaluate the workload on different hardware platforms. This reference solution will help evaluate your required hardware to minimize the cost per workload.</p> <p></p>"},{"location":"index.html#install-platform","title":"Install Platform","text":"<p>Make sure that your platform is included in the supported platform list. To set up the platform, refer to Hardware Setup.</p>"},{"location":"index.html#releases","title":"Releases","text":"<p>For the project release notes, refer to the GitHub* Repository.</p>"},{"location":"index.html#license","title":"License","text":"<p>This project is Licensed under an Apache License.</p>"},{"location":"LICENSE.html","title":"LICENSE","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li>Definitions.</li> </ol> <p>\"License\" shall mean the terms and conditions for use, reproduction,    and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by    the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all    other entities that control, are controlled by, or are under common    control with that entity. For the purposes of this definition,    \"control\" means (i) the power, direct or indirect, to cause the    direction or management of such entity, whether by contract or    otherwise, or (ii) ownership of fifty percent (50%) or more of the    outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity    exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,    including but not limited to software source code, documentation    source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical    transformation or translation of a Source form, including but    not limited to compiled object code, generated documentation,    and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or    Object form, made available under the License, as indicated by a    copyright notice that is included in or attached to the work    (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object    form, that is based on (or derived from) the Work and for which the    editorial revisions, annotations, elaborations, or other modifications    represent, as a whole, an original work of authorship. For the purposes    of this License, Derivative Works shall not include works that remain    separable from, or merely link (or bind by name) to the interfaces of,    the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including    the original version of the Work and any modifications or additions    to that Work or Derivative Works thereof, that is intentionally    submitted to Licensor for inclusion in the Work by the copyright owner    or by an individual or Legal Entity authorized to submit on behalf of    the copyright owner. For the purposes of this definition, \"submitted\"    means any form of electronic, verbal, or written communication sent    to the Licensor or its representatives, including but not limited to    communication on electronic mailing lists, source code control systems,    and issue tracking systems that are managed by, or on behalf of, the    Licensor for the purpose of discussing and improving the Work, but    excluding communication that is conspicuously marked or otherwise    designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity    on behalf of whom a Contribution has been received by Licensor and    subsequently incorporated within the Work.</p> <ol> <li> <p>Grant of Copyright License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    copyright license to reproduce, prepare Derivative Works of,    publicly display, publicly perform, sublicense, and distribute the    Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of    this License, each Contributor hereby grants to You a perpetual,    worldwide, non-exclusive, no-charge, royalty-free, irrevocable    (except as stated in this section) patent license to make, have made,    use, offer to sell, sell, import, and otherwise transfer the Work,    where such license applies only to those patent claims licensable    by such Contributor that are necessarily infringed by their    Contribution(s) alone or by combination of their Contribution(s)    with the Work to which such Contribution(s) was submitted. If You    institute patent litigation against any entity (including a    cross-claim or counterclaim in a lawsuit) alleging that the Work    or a Contribution incorporated within the Work constitutes direct    or contributory patent infringement, then any patent licenses    granted to You under this License for that Work shall terminate    as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the    Work or Derivative Works thereof in any medium, with or without    modifications, and in Source or Object form, provided that You    meet the following conditions:</p> </li> </ol> <p>(a) You must give any other recipients of the Work or    Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices    stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works    that You distribute, all copyright, patent, trademark, and    attribution notices from the Source form of the Work,    excluding those notices that do not pertain to any part of    the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its    distribution, then any Derivative Works that You distribute must    include a readable copy of the attribution notices contained    within such NOTICE file, excluding those notices that do not    pertain to any part of the Derivative Works, in at least one    of the following places: within a NOTICE text file distributed    as part of the Derivative Works; within the Source form or    documentation, if provided along with the Derivative Works; or,    within a display generated by the Derivative Works, if and    wherever such third-party notices normally appear. The contents    of the NOTICE file are for informational purposes only and    do not modify the License. You may add Your own attribution    notices within Derivative Works that You distribute, alongside    or as an addendum to the NOTICE text from the Work, provided    that such additional attribution notices cannot be construed    as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and    may provide additional or different license terms and conditions    for use, reproduction, or distribution of Your modifications, or    for any such Derivative Works as a whole, provided Your use,    reproduction, and distribution of the Work otherwise complies with    the conditions stated in this License.</p> <ol> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,    any Contribution intentionally submitted for inclusion in the Work    by You to the Licensor shall be under the terms and conditions of    this License, without any additional terms or conditions.    Notwithstanding the above, nothing herein shall supersede or modify    the terms of any separate license agreement you may have executed    with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade    names, trademarks, service marks, or product names of the Licensor,    except as required for reasonable and customary use in describing the    origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or    agreed to in writing, Licensor provides the Work (and each    Contributor provides its Contributions) on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or    implied, including, without limitation, any warranties or conditions    of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A    PARTICULAR PURPOSE. You are solely responsible for determining the    appropriateness of using or redistributing the Work and assume any    risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,    whether in tort (including negligence), contract, or otherwise,    unless required by applicable law (such as deliberate and grossly    negligent acts) or agreed to in writing, shall any Contributor be    liable to You for damages, including any direct, indirect, special,    incidental, or consequential damages of any character arising as a    result of this License or out of the use or inability to use the    Work (including but not limited to damages for loss of goodwill,    work stoppage, computer failure or malfunction, or any and all    other commercial damages or losses), even if such Contributor    has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing    the Work or Derivative Works thereof, You may choose to offer,    and charge a fee for, acceptance of support, warranty, indemnity,    or other liability obligations and/or rights consistent with this    License. However, in accepting such obligations, You may act only    on Your own behalf and on Your sole responsibility, not on behalf    of any other Contributor, and only if You agree to indemnify,    defend, and hold each Contributor harmless for any liability    incurred by, or claims asserted against, such Contributor by reason    of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2023 Intel Corporation</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"camera_serial_number.html","title":"Get Serial Number of Intel\u00ae RealSense\u2122 Camera","text":"<p>Do the following to get the serial number of an Intel\u00ae RealSense\u2122 Camera:</p> <ol> <li>Build the RealSense version of dlstreamer Docker image if not done yet:</li> </ol> <pre><code>    make build-dlstreamer-realsense\n</code></pre> <ol> <li> <p>Plug in your Intel\u00ae RealSense\u2122 Camera into the system;</p> </li> <li> <p>Use the makefile target <code>get-realsense-serial-num</code> to get the serial number of your Intel\u00ae RealSense\u2122 Camera:</p> </li> </ol> <pre><code>    $ make get-realsense-serial-num\n</code></pre> <p>You should see a serial number printed out. If you do not see the expected results, check if the Intel\u00ae RealSense\u2122 Camera is plugged in.</p>"},{"location":"environment_variables.html","title":"Environment Variable (EV)","text":"<p>We support environment variables (EVs) as inputs for container that runs inferencing pipeline, we categorize three types of EVs:</p> <pre><code>1. EVs support dlstreamer workload only\n2. EVs support ovms workload only\n3. EVs support both\n</code></pre> EVs for DLStreamerEVs for OVMSEVs for Both <p>This list of EVs specifically supports DLStreamer workloads.</p> Variable Description Values <code>GST_PIPELINE_LAUNCH</code> for launching gst pipeline script file path and name Ex: \"/home/pipeline-server/framework-pipelines/yolov5_pipeline/yolov5s.sh\" <code>GST_DEBUG</code> for running pipeline in gst debugging mode 0, 1 <code>LOG_LEVEL</code> log level to be set when running gst pipeline ERROR, INFO, WARNING, and more <code>AGGREGATE</code> aggregate branches of the gstreamer pipeline, if any, at the end of the pipeline \"\", \"gvametaaggregate name=aggregate\", \"aggregate branch. ! queue\" <code>OUTPUTFORMAT</code> output format gstreamer instructions for the pipeline \"! fpsdisplaysink video-sink=fakesink sync=true --verbose\", \"(render_mode)! videoconvert ! video/x-raw,format=I420 ! gvawatermark ! videoconvert ! fpsdisplaysink video-sink=ximagesink sync=true --verbose\" <code>VA_SURFACE</code> use video analytics surface from the shared memory if applicable \"\", \"! \"video/x-raw(memory <code>PARALLEL_PIPELINE</code> run pipeline in parallel using the tee branch \"\", \"! tee name=branch ! queue\" <code>PARALLEL_AGGRAGATE</code> aggregate parallel pipeline results together, paired use with PARALLEL_PIPELINE \"\", \"! gvametaaggregate name=aggregate ! gvametaconvert name=metaconvert add-empty-results=true ! gvametapublish name=destination file-format=2 file-path=/tmp/results/r$cid_count.jsonl ! fpsdisplaysink video-sink=fakesink sync=true --verbose branch. ! queue !\" <code>DETECTION_OPTIONS</code> extra object detection pipeline instruction parameters \"\", \"gpu-throughput-streams=4 nireq=4 batch-size=1\" <code>CLASSIFICATION_OPTIONS</code> extra classification pipeline instruction parameters \"\", \"reclassify-interval=1 batch-size=1 nireq=4 gpu-throughput-streams=4\" <p>This list of EVs specifically supports OVMS workloads.</p> Variable Description Values <code>PIPELINE_PROFILE</code> for choosing ovms workload's pipeline profile to run use <code>make list-profiles</code> to see Values <p>This list of EVs supports both DLStreamer and OVMS workloads.</p> Variable Description Values <code>RENDER_MODE</code> for displaying pipeline and overlay CV metadata 1, 0 <code>LOW_POWER</code> for running pipelines using GPUs only 1, 0 <code>CPU_ONLY</code> for overriding inference to be performed on CPU only 1, 0 <code>STREAM_DENSITY_MODE</code> for starting pipeline stream density testing 1, 0 <code>STREAM_DENSITY_FPS</code> for setting stream density target fps value Ex: 15.0 <code>STREAM_DENSITY_INCREMENTS</code> for setting incrementing number of pipelines for running stream density Ex: 1 <code>AUTO_SCALE_FLEX_140</code> allow workload to manage autoscaling 1, 0 <code>DEVICE</code> for setting device to use for pipeline run \"GPU\", \"CPU\", \"AUTO\", \"MULTI <code>OCR_DEVICE</code> optical character recognition device \"CPU\", \"GPU\" <code>PRE_PROCESS</code> pre process command to add for inferencing \"pre-process-backend=vaapi-surface-sharing\", \"pre-process-backend=vaapi-surface-sharing pre-process-config=VAAPI_FAST_SCALE_LOAD_FACTOR=1\""},{"location":"environment_variables.html#applying-ev-to-run-pipeline","title":"Applying EV to Run Pipeline","text":"<p>EV can be applied in two ways |</p> <pre><code>1. as parameter input to run.sh script\n2. in the env files\n</code></pre> <p>The input parameter will override the one in the env files if both are used.</p>"},{"location":"environment_variables.html#ev-as-input-parameter","title":"EV as input parameter","text":"<p>EV as input parameter to pipeline run, here is an example |</p> <pre><code>CPU_ONLY=1 sudo -E ./run.sh --workload dlstreamer --platform core --inputsrc rtsp |//127.0.0.1 |8554/camera_0 --ocr_disabled --barc\node_disabled\n</code></pre>"},{"location":"environment_variables.html#editing-the-env-files","title":"Editing the Env Files","text":"<p>EV can be configured for advanced user in <code>configs/dlstreamer/framework-pipelines/yolov5_pipeline/</code></p> <pre><code>    |`yolov5-cpu.env` file for running pipeline in core system\n    |`yolov5-gpu.env` file for running pipeline in gpu or multi\n</code></pre> <p>these two files currently hold the default values.</p>"},{"location":"faq.html","title":"Frequently Asked Questions","text":""},{"location":"faq.html#what-are-the-platform-requirements","title":"What are the platform requirements?","text":"<p>For optimal hardware, refer to the platform guide.</p>"},{"location":"faq.html#what-are-the-software-prerequisites","title":"What are the software prerequisites?","text":"<p>At a minimum, you will need Docker* 23.0 or later. For more details, refer to the hardware setup.</p>"},{"location":"faq.html#how-do-i-download-the-models","title":"How do I download the models?","text":"<p>The models are downloaded automatically when the benchmark script <code>benchmark.sh</code> is run. For more details on downloading models, refer to Set up Pipeline.</p>"},{"location":"faq.html#how-do-i-simulate-rtsp-cameras","title":"How do I simulate RTSP cameras?","text":"<p>You can use the camera simulator script <code>camera-simulator.sh</code> to run simulated RTSP camera streams. For more details on the script, refer to Run Pipeline page.</p>"},{"location":"faq.html#how-do-i-download-video-files-for-benchmarking","title":"How do I download video files for benchmarking?","text":"<p>You can download your own media file or use the the provided <code>download_sample_videos.sh</code> script to download an existing media file. For more details, refer to Benchmark Pipeline.</p>"},{"location":"faq.html#how-do-i-run-different-types-of-pipelines","title":"How do I run different types of pipelines?","text":"<p>For details on running different types of pipelines, refer to Benchmark Pipeline.</p>"},{"location":"hardwaresetup.html","title":"System Setup","text":""},{"location":"hardwaresetup.html#prerequisites","title":"Prerequisites","text":"<p>To build the Intel\u00ae Automated Self-Checkout Reference Package, you need:</p> <ul> <li>Ubuntu LTS (22.04 or 20.04)</li> <li>Docker</li> <li>Docker Compose v2 (Required, if using docker compose feature)</li> <li>Git</li> </ul> <p>Click the links for corresponding set up instructions.</p>"},{"location":"hardwaresetup.html#hardware-dependent-installation","title":"Hardware Dependent Installation","text":"11th/12th Gen Intel\u00ae Core\u2122 ProcessorIntel\u00ae Xeon\u00ae ProcessorIntel\u00ae Data Center GPU Flex 140/170Intel\u00ae Arc\u2122 <ol> <li> <p>Download Ubuntu 20.04 and follow these installation steps.</p> </li> <li> <p>Install Docker* Engine</p> <p>Note</p> <p>To avoid typing <code>sudo</code> when running the Docker command, follow these steps.</p> </li> <li> <p>[Optional] Install Docker Compose v2, if using the docker compose feature</p> </li> <li> <p>Install Git</p> </li> <li> <p>Set up the pipeline</p> </li> </ol> <ol> <li> <p>Download Ubuntu 22.04 and follow these installation steps.</p> </li> <li> <p>Install Docker Engine</p> </li> <li> <p>[Optional] Install Docker Compose v2, if using the docker compose feature</p> </li> <li> <p>Install Git</p> </li> <li> <p>Set up the pipeline</p> </li> </ol> <ol> <li> <p>Download Ubuntu 22.04 and follow these installation steps.</p> </li> <li> <p>Update the Kernel</p> <p>Warning</p> <p>After the kernel is updated, <code>apt-get install</code> might not work due to the unsupported kernel versions that were installed. To resolve this issue, do the following:</p> <ol> <li> <p>Find all the installed kernels</p> <pre><code>    dpkg --list | grep -E -i --color 'linux-image|linux-headers'\n</code></pre> </li> <li> <p>Then remove the unsupported kernels. The example below will remove the installed kernel 5.19:</p> <pre><code>    sudo apt-get purge -f 'linux--5.19'\n</code></pre> </li> </ol> </li> <li> <p>Install Docker Engine</p> </li> <li> <p>[Optional] Install Docker Compose v2, if using the docker compose feature</p> </li> <li> <p>Install Git</p> </li> <li> <p>Set up the pipeline</p> </li> </ol> <ol> <li> <p>Download Ubuntu 20.04 and follow these installation steps.</p> </li> <li> <p>Update the Kernel</p> <p>Warning</p> <p>After the kernel is updated, <code>apt-get install</code> might not work due to the unsupported kernel versions that were installed. To resolve this issue, do the following:</p> <ol> <li> <p>Find all the installed kernels</p> <pre><code>    dpkg --list | grep -E -i --color 'linux-image|linux-headers'\n</code></pre> </li> <li> <p>Then remove the unsupported kernels. The example below will remove the installed kernel 5.19:</p> <pre><code>    sudo apt-get purge -f 'linux--5.19'\n</code></pre> </li> </ol> </li> <li> <p>Install Docker Engine</p> </li> <li> <p>[Optional] Install Docker Compose v2, if using the docker compose feature</p> </li> <li> <p>Install Git</p> </li> <li> <p>Set up the pipeline</p> </li> </ol>"},{"location":"pipelinebenchmarking.html","title":"Computer Vision Pipeline Benchmarking","text":"<p>You can benchmark pipelines with a collection of scripts to get the pipeline performance metrics such as video processing in frame-per-second (FPS), memory usage, power consumption, and so on.</p>"},{"location":"pipelinebenchmarking.html#prerequisites","title":"Prerequisites","text":"<p>Before benchmarking, make sure you set up the pipeline.</p>"},{"location":"pipelinebenchmarking.html#steps-to-benchmark-computer-vision-pipelines","title":"Steps to Benchmark Computer Vision Pipelines","text":"<ol> <li> <p>Build the benchmark Docker* images     Benchmark scripts are containerized inside Docker. The following table lists the commands for various platforms. Choose and run the command corresponding to your hardware configuration.</p> Platform Docker Build Command Check Success Intel\u00ae integrated and Arc\u2122 GPUs <pre>cd benchmark-scriptsmake build-benchmarkmake build-igt</pre> Docker images command to show both <code>benchmark:dev</code> and <code>benchmark:igt</code> images Intel\u00ae Flex GPUs <pre>cd benchmark-scriptsmake build-benchmarkmake build-xpu</pre> Docker images command to show both <code>benchmark:dev</code> and <code>benchmark:xpu</code> images <p>Warning</p> <p>Build command may take a while, depending on your internet connection and machine specifications.</p> </li> <li> <p>Determine the appropriate parameters for</p> <ul> <li>Input source type</li> <li>Platform</li> <li>Workload</li> <li>Profile for OVMS workload</li> </ul> </li> <li> <p>Run the benchmark. The <code>benchmark.sh</code> shell script is in the base/benchmark_scripts directory. </p> <pre><code>cd benchmark_scripts\nsudo ./benchmark.sh --pipelines &lt;number of pipelines&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt;\n</code></pre> <p>Note</p> <p>The <code>benchmark.sh</code> can either benchmark a specific number of pipelines or benchmark stream density based on the desired FPS. See Additional Benchmark Examples for more options.</p> </li> </ol>"},{"location":"pipelinebenchmarking.html#input-source-type","title":"Input Source Type","text":"<p>The benchmark script can take either of the following video input sources:</p> <ul> <li> <p>Real Time Streaming Protocol (RTSP) <pre><code>--inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre></p> <p>Note</p> <p>Using RTSP source with <code>benchmark.sh</code> will automatically run the camera simulator. The camera simulator will start an RTSP stream for each video file in the sample-media folder.</p> </li> <li> <p>USB Camera <pre><code>--inputsrc /dev/video&lt;N&gt;, where N is 0 or an integer\n</code></pre></p> </li> <li> <p>Intel\u00ae RealSense\u2122 Camera <pre><code>--inputsrc &lt;RealSense camera serial number&gt;\n</code></pre>     To know the serial number of the Intel\u00ae RealSense\u2122 Camera, refer to Get Serial Number of Intel\u00ae RealSense\u2122 Camera.</p> </li> <li> <p>Video File <pre><code>--inputsrc file:my_video_file.mp4\n</code></pre></p> <p>Note</p> <p>Video files must be in the sample-media folder, so that the Docker container can access the files. You can provide your own video files or download a sample video file using the script download_sample_videos.sh.</p> </li> </ul>"},{"location":"pipelinebenchmarking.html#platform","title":"Platform","text":"<ul> <li> <p>Intel\u00ae Core\u2122 Processor</p> <ul> <li><code>--platform core.x</code> if GPUs are available, then replace this parameter with targeted GPUs such as core (for all GPUs), core.0, core.1, and so on</li> <li><code>--platform core</code> will evenly distribute and utilize all available core GPUs</li> </ul> </li> <li> <p>Intel\u00ae Xeon\u00ae Scalable Processor</p> <ul> <li><code>--platform xeon</code> will use the Xeon CPU for the pipelines</li> </ul> </li> <li> <p>DGPU (Intel\u00ae Data Center GPU Flex 140,  Intel\u00ae Data Center GPU Flex 170, and Intel\u00ae Arc\u2122 Setup)</p> <ul> <li><code>--platform dgpu.x</code> replace this parameter with targeted GPUs such as dgpu (for all GPUs), dgpu.0, dgpu.1, and so on</li> <li><code>--platform dgpu</code> will evenly distribute and utilize all available dgpus</li> </ul> </li> </ul>"},{"location":"pipelinebenchmarking.html#benchmark-specified-number-of-pipelines","title":"Benchmark Specified Number of Pipelines","text":"<p>The primary purpose of benchmarking with a specified number of pipelines is to discover the performance and system requirements for a given use case.</p> <p>Example</p> <p>Here is an example of running benchmarking pipelines with specified number of pipelines: <pre><code>sudo ./benchmark.sh --pipelines &lt;number of pipelines&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt;\n</code></pre></p> <p>where, the configurable input parameters include: </p> <ul> <li><code>--performance_mode</code> configures the scaling governor of the system. Supported modes are performance and powersave (default).</li> <li><code>--logdir</code> configures the benchmarking output directory</li> <li><code>--duration</code> configures the duration, in number of seconds, the benchmarking will run</li> <li><code>--init_duration</code> configures the duration, in number of seconds, to wait for system initialization before the benchmarking metrics or data collection begins</li> </ul> <p>Example</p> <p>You can run multiple pipeline benchmarking with different configurations before consolidating all pipeline output results.</p> <p>To get the consolidated pipeline results, run the following <code>make</code> command: <pre><code>make consolidate ROOT_DIRECTORY=&lt;output dir&gt;\n</code></pre> This command will consolidate the performance metrics that exist in the specified <code>ROOT_DIRECTORY</code>. </p> <p>Here is an example of consolidated output: </p> <p>Success</p> <p>Output of <code>Consolidate_multiple_run_of_metrics.py</code></p> <pre><code>,Metric,data\n0,Total Text count,0\n1,Total Barcode count,2\n2,Camera_1 FPS,15.0\n3,Camera_0 FPS,15.0\n4,CPU Utilization %,16.548\n5,Memory Utilization %,21.162\n6,Disk Read MB/s,0.0\n7,Disk Write MB/s,0.025\n8,S0 Memory Bandwidth Usage MB/s,1872.632\n9,S0 Power Draw W,27.502\n10,GPU_0 VDBOX0 Utilization %,0.0\n11,GPU_0 GPU Utilization %,17.282\n</code></pre>"},{"location":"pipelinebenchmarking.html#benchmark-stream-density","title":"Benchmark Stream Density","text":"<p>Benchmarking a pipeline can also discover the maximum number of workloads or streams that can be ran in parallel for a given target FPS. This information is useful to determine the hardware required to achieve the desired performance for input sources.</p> <p>To run the stream density functionality: <pre><code>sudo ./benchmark.sh --stream_density &lt;target FPS&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt;\n</code></pre></p> <p>Note</p> <p>It is recommended to set <code>--stream_density</code> to a value lesser than your target FPS to account for real world variances in hardware readings.</p> <p>Note</p> <p>Because stream density requires a continuous video stream it is recommended to use an RTSP stream, USB camera, or RealSense camera. If these options are not available you can use the camera simulator to continuously loop through a video file as an RTSP stream.</p>"},{"location":"pipelinebenchmarking.html#workload","title":"Workload","text":"<p>We are currently supporting 2 types of workloads:     1. dlstreamer     2. ovms</p> <p>These are the input value for <code>--workload</code> parameter for benchmark.sh script. The default value for <code>--workload</code> parameter is <code>dlstreamer</code> in case it is not provided when running benchmark.sh script.</p>"},{"location":"pipelinebenchmarking.html#benchmark-specified-profile-for-ovms","title":"Benchmark Specified Profile for OVMS","text":"<p>For running ovms workload, we are supporting different programming languages and different models. You may specify language choice and model input. Then you may prefix benchmark script run command with specific profile.</p> <p>An example of stream density benchmark script in golang: <pre><code>PIPELINE_PROFILE=\"grpc_go\" sudo -E ./benchmark.sh --stream_density 14.9 --logdir mytest/data --duration 60 --init_duration 20 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 --workload ovms\n</code></pre></p> <p>An example of stream density benchmark script in python: <pre><code>PIPELINE_PROFILE=\"grpc_python\" sudo -E ./benchmark.sh --stream_density 14.9 --logdir mytest/data --duration 60 --init_duration 60 --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 --workload ovms\n</code></pre> If prefix is not provided, then the default value is \"grpc_python\".</p>"},{"location":"pipelinebenchmarking.html#additional-benchmark-examples","title":"Additional Benchmark Examples","text":"<p>Run decode+pre-processing+object detection (Yolov5s 416x416) only pipeline:</p> <pre><code>sudo ./benchmark.sh --pipelines &lt;number of pipelines&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;4k rtsp stream with 5 objects&gt; --ocr_disabled --barcode_disabled --classification_disabled\n</code></pre> <pre><code>sudo ./benchmark.sh --stream_density &lt;target FPS&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt; --ocr_disabled --barcode_disabled --classification_disabled\n</code></pre> <p>Run decode+pre-processing+object detection (Yolov5s 416x416) + efficientnet-b0 (224x224) only pipeline:</p> <pre><code>sudo ./benchmark.sh --pipelines &lt;number of pipelines&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;4k rtsp stream with 5 objects&gt; --ocr_disabled --barcode_disabled\n</code></pre> <pre><code>sudo ./benchmark.sh --stream_density &lt;target FPS&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt; --ocr_disabled --barcode_disabled\n</code></pre> <p>Run  decode+pre-processing+object detection (Yolov5s 416x416) + efficientnet-b0 (224x224) + optical character recognition + barcode detection and decoding</p> <pre><code>sudo ./benchmark.sh --pipelines &lt;number of pipelines&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;4k rtsp stream with 5 objects&gt; --ocr 5 GPU\n</code></pre> <pre><code>sudo ./benchmark.sh --stream_density &lt;target FPS&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform &lt;core|xeon|dgpu.x&gt; --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt; --ocr 5 GPU\n</code></pre> <p>Run Flex140 optimized decode+pre-processing+object detection (Yolov5s 416x416) + efficientnet-b0 (224x224) + optical character recognition + barcode detection and decoding: <pre><code>sudo ./benchmark.sh --pipelines 2 --logdir &lt;output dir&gt;/data1 --init_duration 30 --duration 120 --platform dgpu.0 --inputsrc &lt;4k rtsp stream with 5 objects&gt; --ocr 5 GPU\n\nsudo ./benchmark.sh --pipelines 2 --logdir &lt;output dir&gt;/data1 --init_duration 30 --duration 120 --platform dgpu.1 --inputsrc &lt;4k rtsp stream with 5 objects&gt; --ocr 5 GPU\n</code></pre></p> <pre><code>sudo ./benchmark.sh --stream_density &lt;target FPS&gt; --logdir &lt;output dir&gt;/data --init_duration 30 --duration 120 --platform dgpu --inputsrc &lt;ex:4k rtsp stream with 10 objects&gt; --ocr 5 GPU\n</code></pre>"},{"location":"pipelinebenchmarking.html#appendix-benchmark-helper-scripts","title":"Appendix: Benchmark Helper Scripts","text":"<ul> <li> <p><code>camera-simulator.sh</code>: This script starts the camera simulator. Create two folders named camera-simulator and sample-media. Place <code>camera-simulator.sh</code> in the camera-simulator folder. Manually copy the video files to the sample-media folder or run the <code>download_sample_videos.sh</code> script to download sample videos. The <code>camera-simulator.sh</code> script will start a simulator for each .mp4 video that it finds in the sample-media folder and will enumerate them as camera_0, camera_1, and so on. Make sure that the path to the <code>camera-simulator.sh</code> script is mentioned correctly in the <code>camera-simulator.sh</code> script.  </p> </li> <li> <p><code>stop_server.sh</code>: This script stops and removes all Docker containers closing the pipelines.</p> </li> </ul>"},{"location":"pipelinerun.html","title":"Run Pipeline","text":""},{"location":"pipelinerun.html#prerequisites","title":"Prerequisites","text":"<p>Before running, set up the pipeline.</p>"},{"location":"pipelinerun.html#overview","title":"Overview","text":"<p>When the pipeline is run, the <code>run.sh</code> script starts the service and performs inferencing on the selected input media. The output of running the pipeline provides the inference results for each frame based on the media source such as text, barcode, and so on, as well as the frames per second (FPS). Pipeline run provides many options in media type, system process platform type, and additional optional parameters. These options give you the opportunity to compare what system process platform is better for your need.</p>"},{"location":"pipelinerun.html#start-pipeline","title":"Start Pipeline","text":"<p>You can run the pipeline script, <code>run.sh</code>, with the following input parameters:</p> <ol> <li>Media type<ul> <li>Camera Simulator using RTSP</li> <li>Intel\u00ae RealSense\u2122 Camera</li> <li>USB Camera</li> <li>Video File</li> </ul> </li> <li>Platform<ul> <li>core</li> <li>dgpu.0</li> <li>dgpu.1</li> <li>xeon</li> </ul> </li> <li>Optional parameters</li> </ol> <p>Run the command based on specific requirements. Select choices for #1, #2, #3 above to start the pipeline run, see details section below.</p>"},{"location":"pipelinerun.html#check-successful-pipeline-run","title":"Check successful pipeline run","text":"<p>Once pipeline run has started, you will expect containers to be running, see check for pipeline run success; For a successful run, you should expect results/ directory filled with log files and you can watch these log files grow, see sample output log files.</p>"},{"location":"pipelinerun.html#stop-pipeline-run","title":"Stop pipeline run","text":"<p>You can call <code>make clean</code> to stop the pipeline container, hence the results directory log files will stop growing. Below is the table of make commands you can call to clean things up per your needs:</p> Clean Containers Options Command clean simulator containers <pre>make clean-simulator</pre> clean sco-* containers <pre>make clean</pre> clean simulator and self-checkout containers and results/ folder <pre>make clean-all</pre>"},{"location":"pipelinerun.html#run-pipeline-with-different-input-sourceinputsrc-types","title":"Run pipeline with different input source(inputsrc) types","text":"<p>Use run.sh to run the pipeline, here is the table of basic scripts for each combination:</p> Input source Type scripts Simulated camera <code>sudo ./run.sh --platform core|xeon|dgpu.x --inputsrc rtsp://127.0.0.1:8554/camera_0</code> RealSense camera <code>sudo ./run.sh --platform core|xeon|dgpu.x --inputsrc  --realsense_enabled USB camera <code>sudo ./run.sh --platform core|xeon|dgpu.x --inputsrc /dev/video0</code> WebCam as RTSP <code>sudo ./run.sh --platform core|xeon|dgpu.x --inputsrc rtsp://127.0.0.1:8554/cam</code> Video file <code>sudo ./run.sh --platform core|xeon|dgpu.x --inputsrc file:my_video_file.mp4</code> <p>Note</p> <p>For a simulated camera as the input source, run camera simulator first.</p> <p>Note</p> <p>For a webcam to RTSP as the input source, convert webcam to RTSP.</p> <p>Note</p> <p>The value of x in <code>dgpu.x</code> can be 0, 1, 2, and so on depending on the number of discrete GPUs in the system.</p> <p>Note</p> <p>Follow these steps to see the output formats supported by your USB camera.</p>"},{"location":"pipelinerun.html#optional-parameters","title":"Optional Parameters","text":"<p>The following are the optional parameters that you can provide as input to <code>run.sh</code>. Note that these parameters would affect the performance of the pipeline. </p> <ul> <li> <p><code>--classification_disabled</code>: Disables the classification process of image extraction. By default, the classification is enabled. </p> </li> <li> <p><code>--ocr_disabled</code>: Disables optical character recognition (OCR). By default, OCR is enabled. </p> </li> <li> <p><code>--ocr</code>: Provides the OCR frame internal value, such as <code>--ocr 5 GPU</code>. The default recognition interval value is 5. Note device equal to CPU is not supported when executing with a discrete GPU.</p> </li> <li> <p><code>--barcode_disabled</code>: Disables barcode detection. By default, barcode detection is enabled.</p> </li> <li> <p><code>--realsense_enabled</code>: Uses the Intel\u00ae RealSense\u2122 Camera and provides the 12-digit serial number of the camera as an input to the <code>run.sh</code> script.</p> </li> <li> <p><code>--barcode</code>: Provides barcode detection frame internal value such as <code>--barcode 5</code>, default recognition interval value is 5.</p> </li> <li> <p><code>--color-width</code>, <code>color-height</code>, and <code>color-framerate</code>: Allows you to customize the settings of the color frame output from the Intel\u00ae RealSense\u2122 Cameras. This parameter will overwrite the default value of RealSense gstreamer. Use <code>rs-enumerate-devices</code> to look up the camera's color capability.</p> </li> </ul> <p>Here is an example to run a RealSense pipeline with optional parameters: <pre><code>sudo ./run.sh --platform core --inputsrc &lt;serial_number&gt; --realsense_enabled --color-width 1920 --color-height 1080 --color-framerate 15 --ocr 5 CPU\n</code></pre></p>"},{"location":"pipelinerun.html#environment-variables","title":"Environment variables","text":"<p>When running run.sh script, we support environment variables as input for containers. Here is a list of environment variables and how to apply them</p>"},{"location":"pipelinerun.html#status-of-running-a-pipeline","title":"Status of Running a Pipeline","text":"<p>When you run the pipeline, the containers will run.</p> <p>Check if the pipeine run is successful: </p> <pre><code>docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}'\n</code></pre> <p>Success</p> <p>Your output for DLStreamer is as follows:</p> IMAGE STATUS NAMES dlstreamer:dev Up 9 seconds automated-self-checkout0 <p>Success</p> <p>If the run is successful, the results directory will contain the log files. Check the inference results and use case performance:</p> <pre><code>ls -l results\n</code></pre> <p>The results directory contains three types of log files:</p> <pre><code>- **pipeline#.log** files for each pipeline/workload that is running and is the pipeline/workload current FPS (throughput) results.\n- **r#.jsonl** for each of pipeline/workload that is running and is the pipeline/workload inference results.\n- **gst-launch_device_#.log** for gst-launch console output helping for debug; the `device` in file name can be core|dgpu|xeon.\n</code></pre> <p>The # suffixed to each log file name corresponds to each pipeline run index number.</p>"},{"location":"pipelinerun.html#sample-output","title":"Sample output","text":"<p>Here are the sample outputs:</p> <p>results/r0.jsonl sample:</p> <p>The results/r0.jsonl file lists all the metadata detected in each frame such as text, barcode, and so on. The output is not in human readable format and are meant to be parsed by scripts. </p> <pre><code>{\n    \"objects\": [\n        {\n            \"classification_layer_name:efficientnet-b0/model/head/dense/BiasAdd/Add\": {\n                \"confidence\": 10.4609375,\n                \"label\": \"n07892512 red wine\",\n                \"label_id\": 966,\n                \"model\": {\n                    \"name\": \"efficientnet-b0\"\n                }\n            },\n            \"detection\": {\n                \"bounding_box\": {\n                    \"x_max\": 0.7873224129905809,\n                    \"x_min\": 0.6722826382852345,\n                    \"y_max\": 0.7966044796082201,\n                    \"y_min\": 0.14121232192034938\n                },\n                \"confidence\": 0.8745479583740234,\n                \"label\": \"bottle\",\n                \"label_id\": 39\n            },\n            \"h\": 472,\n            \"id\": 1,\n            \"region_id\": 425,\n            \"roi_type\": \"bottle\",\n            \"w\": 147,\n            \"x\": 861,\n            \"y\": 102\n        },\n        {\n            \"detection\": {\n                \"bounding_box\": {\n                    \"x_max\": 0.7873224129905809,\n                    \"x_min\": 0.6722826382852345,\n                    \"y_max\": 0.7966044796082201,\n                    \"y_min\": 0.14121232192034938\n                },\n                \"confidence\": 0.8745479583740234,\n                \"label\": \"bottle\",\n                \"label_id\": 39\n            },\n            \"h\": 472,\n            \"region_id\": 425,\n            \"roi_type\": \"bottle\",\n            \"w\": 147,\n            \"x\": 861,\n            \"y\": 102\n        }\n    ],\n    \"resolution\": {\n        \"height\": 720,\n        \"width\": 1280\n    },\n    \"timestamp\": 133305309\n}\n</code></pre> <p>results/pipeline0.log sample*:</p> <p>The results/pipeline0.log file lists FPS during pipeline run. </p> <pre><code>27.34\n27.34\n27.60\n27.60\n28.30\n28.30\n28.61\n28.61\n28.48\n28.48\n...\n</code></pre> <p>The results directory is volume mounted to the pipeline container. The log files within the results increase as the pipeline continues to run. You can stop the pipeline and the containers that are running.</p> <p>Failure</p> <p>Review the console output for errors if you do not see all the Docker* containers. Sometimes dependencies fail to resolve. Address obvious issues and rerun the pipeline.</p>"},{"location":"pipelinerun.html#stop-pipeline","title":"Stop Pipeline","text":"<p>Run the following command to stop the pipeline and the containers that are running:</p> <pre><code>./stop_all_docker_containers.sh\n</code></pre>"},{"location":"pipelinesetup.html","title":"Pipeline Setup","text":"<p>Pipelines may either be setup to run using Open Vino Model Server (OVMS) (default) or DLStreamer. </p> <ul> <li>For setting up pipelines with OVMS (recommended), click here</li> <li>For setting up pipelines with DLStreamer, click here</li> </ul> <p>Warning</p> <p>Support for DL Streamer may be deprecated in a future release.</p>"},{"location":"pipelinesetup_dlstreamer.html","title":"Set up Pipeline","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone  https://github.com/intel-retail/automated-self-checkout.git &amp;&amp; cd ./automated-self-checkout\n</code></pre> </li> <li> <p>For use cases that support benchmarking, build the benchmark containers:</p> <pre><code>cd benchmark-scripts\nmake build-all\n</code></pre> </li> <li> <p>Download the models manually (optional).</p> <p>Note</p> <p>When <code>run.sh</code> the Model downloader script is automatically called.</p> <pre><code>sh modelDownload.sh\n</code></pre> <p>Warning</p> <p>This step might take a while as the script will download several models for the first time.</p> <p>Note</p> <p>For the list of pre-trained models, refer to the Model List.</p> </li> <li> <p>Build the reference design Docker* images. You must build the provided component services and create local docker images. The following table lists the build command for various platforms. Choose and run the command corresponding to your platforms or hardware.</p> Platform Docker Build Command Check Success Intel platforms using DLStreamer <pre>make build-dlstreamer</pre> docker images command to show sdlstreamer:dev Intel platforms using DLStreamer and Realsense <pre>make build-dlstreamer-realsense</pre> docker images command to show sdlstreamer:realsense <p>Warning</p> <p>Build command may take a while, depending on your internet connection and machine specifications.</p> <p>Note</p> <p>If the build command succeeds, you will see all the built Docker images files as indicated in the Check Success column. If the build fails, check the console output for errors. The dependencies might have been unable to resolve. Address the issue and repeat step 2.</p> <p>Proxy</p> <p>If the docker build system requires a proxy network, just set your proxy env standard way on your terminal as below and make build: <pre><code>export HTTP_PROXY=\"http://your-proxy-url.com:port\"\nexport HTTPS_PROXY=\"https://your-proxy-url.com:port\"\nmake build-all\n</code></pre></p> </li> </ol>"},{"location":"pipelinesetup_dlstreamer.html#next-steps","title":"Next Steps","text":"<p>Run a pipeline or run a benchmark for a pipeline.</p>"},{"location":"platforms.html","title":"Supported Platforms","text":"<p>Following are the list of supported platforms:</p>"},{"location":"platforms.html#cpu","title":"CPU","text":"<ul> <li> <p>11th Gen Intel\u00ae Core\u2122 i5 Processor/11th Gen Intel\u00ae Core\u2122 i7 Processor</p> </li> <li> <p>12th Gen Intel\u00ae Core\u2122 i5 Processor/12th Gen Intel\u00ae Core\u2122 i7 Processor</p> </li> <li> <p>Intel\u00ae Xeon\u00ae Platinum 8351N Processor</p> </li> </ul>"},{"location":"platforms.html#gpu","title":"GPU","text":"<ul> <li> <p>Intel\u00ae Data Center GPU Flex 140</p> </li> <li> <p>Intel\u00ae Data Center GPU Flex 170</p> </li> <li> <p>Intel\u00ae Arc\u2122 A770M Graphics</p> </li> </ul> <p>To set up the platform, refer to Hardware Setup.</p>"},{"location":"query_usb_camera.html","title":"Query USB Camera","text":"<ol> <li> <p>Make sure a USB camera is plugged into the system</p> </li> <li> <p>Install the necessary libraries</p> <pre><code>sudo apt update\nsudo apt install v4l-utils -y\n</code></pre> </li> <li> <p>List available video cameras </p> <pre><code>ls -l /dev/vid*\n</code></pre> <p>Note</p> <p>To get information about the development video ids, check the </p> </li> <li> <p>Execute a video, from the available list, for more information</p> <pre><code>v4l2-ctl --list-formats-ext -d /dev/video0\n</code></pre> <p>Note</p> <p>Here is information on how to .</p> </li> </ol> <p>Example</p> <p>Here is an example to run the pipeline with a USB camera on video0 for the core system: <pre><code>sudo ./run.sh --platform core --inputsrc /dev/video0\n</code></pre></p>"},{"location":"references.html","title":"References","text":""},{"location":"references.html#libraries","title":"Libraries","text":"Library Link Intel\u00ae Deep Learning Streamer (Intel\u00ae DL Streamer) https://github.com/dlstreamer/dlstreamer gstreamer plugin for Intel\u00ae RealSense\u2122 Cameras https://github.com/G2020sudo/realsense-gstreamer Libraries for Intel\u00ae RealSense\u2122 Cameras https://github.com/gwen2018/librealsense"},{"location":"references.html#components","title":"Components","text":"Component Link gstreamer https://gstreamer.freedesktop.org/ Intel\u00ae RealSense\u2122 Technology https://www.intel.com/content/www/us/en/architecture-and-technology/realsense-overview.html"},{"location":"releasenotes.html","title":"Releases","text":"<p>Release v1.0.1</p> <p>Release v1.5.0</p>"},{"location":"roadmap.html","title":"Roadmap","text":"Roadmap Timeline Notes Now <ul> <li>Computer Vision Pipeline             A single service that processes a streaming video and runs inferencing across products placed on a self-checkout station.</li> <li>Benchmark Script           A script that allows the end user to reproduce our benchmark results. The script provides capabilities to run benchmarks on partner models.</li> <li>Benchmark Results           Detailed benchmark results across GPU, CPU and GPU hardware SKUs</li> </ul> <p>Context</p> <p>Automation is rapidly becoming the key transformational strategy for retailers. Age old problems such as inventory control, planogram compliance, store security, store operations, warehousing are areas that can benefit from automation that is predominantly driven by computer vision and AI. Whilst computer vision-based solutions exist for many of these problems, not many have moved beyond pilots and scaled across supermarkets due to various technical and business reasons. We are looking to address the use of computer vision across a multiple set of use cases in the retail space. The first of these use cases will be to help address the scalability of vision enabled self-checkout solutions. This will be followed by use cases such as Loss Prevention, AI assisted Shopping Carts, Autonomous Stores and many more in the future as the retail landscape evolves. The goal of this roadmap is to provide key ingredients and easy decision making to our partners on their journey to build and deploy these use cases at scale.</p> <p>High-level Focus</p>         Vision enabled use cases will need to address four fundamental areas to build and deploy solutions at scale.         <ul> <li>Camera Management           Cameras are a critical piece of the infrastructure, providing both video streams and images to be used across these use cases. We will be focusing on providing a consistent mechanism of onboarding different types of cameras and managing its lifecycle.</li> <li>Computer Vision Pipeline           Workload deployment options and choices of frameworks makes the vision pipeline complex. We will focus our breaking down the pipeline into its individual services and making it easier to deploy and run across distributed architecture.</li> <li>Hardware Recommendation           There are many unknowns when moving from a pilot to production and especially during the scale phase of the project. For every use case, it is important to know exactly what infrastructure is required to deploy and scale the solution.           We plan to remove all the guess work around what hardware is required to run these workloads.</li> <li> Deployment with ISVs            The time taken to operationalize new AI-based software in a new environment is often long. Working with our partners, we would focus our efforts in reducing the time to production.</li> </ul> <p>Out-of-Scope</p>       There are several items that will not be considered as part of this reference implementation. This is not an exhaustive list, but these are core exclusions as they are not our differentiators:       <ul> <li>We will not build or recommend AI models</li> <li>We will not build end-to-end solutions</li> <li>We will not advocate for a specific software deployment architecture</li> </ul> Next <ul> <li>Distributed Architecture         Separating out media pre-processing, AI inferencing and post-processing as completely independent services. Providing a mechanism to deploy and run the individual services across distributed heterogeneous compute. Publish all events to an Enterprise Service Bus (ESB).</li> <li>Update Benchmark script         Update the script to reflect the new distributed architecture.</li> <li>Benchmark Results         Benchmark results across CPU and GPU and GPU only hardware SKUs (previous results updated and new SKUs added)</li> </ul> Later <ul> <li>Model Drift and updates         Monitor model accuracy and rectify model drift. Add new models in live systems.</li> <li>Edge Training         Provide a mechanism to do localized instore training to ensure new products are identified without delay.</li> <li>Hierarchical model support         Provide a mechanism to have a hierarchy of models that gets chosen and executed based on initial segmentation.</li> <li>Dev Cloud Support         Provide a cloud environment for partners to access the hardware and run benchmarks.</li> <li>Camera Management         Provide a mechanism to onboard and drive the camera lifecycle across the store.</li> </ul> <p>Disclaimer: The roadmap is for informational purposes only and is subject to change.</p> <p>Help Drive Our Roadmap Priorities We want to drive our roadmap by building the most valuable assets/ingredients to our partners. Our roadmap will be public and will provide complete transparency to ensure we receive continuous feedback from our partners. Raise any issues and requirements to help us guide our roadmap priorities, and in doing so we can drive the retail vertical forward. </p> <p>Open an issue on GitHub to report a problem or to provide feedback.</p>"},{"location":"run_camera_simulator.html","title":"Camera Simulator","text":""},{"location":"run_camera_simulator.html#overview","title":"Overview","text":"<p>If you do not have a camera device plugged into the system, run the camera simulator to view the pipeline analytic results based on a sample video file to mimic real time camera video. You can also use the camera simulator to infinitely loop through a video file for consistent benchmarking. For example, if you want to validate whether the performance is the same for 6 hours, 12 hours, and 24 hours, looping the same video should produce the same results regardless of the duration.</p>"},{"location":"run_camera_simulator.html#run-the-camera-simulator","title":"Run the Camera Simulator","text":"<p>Follow the steps below to run the camera simulator:</p> <ol> <li> <p>Download the video files to the sample-media directory:     <pre><code>cd benchmark-scripts;\n./download_sample_videos.sh;\ncd ..;\n</code></pre></p> <p>Example</p> <p>This example downloads a sample video for 1080p@15fps. <pre><code>   cd benchmark-scripts;\n   ./download_sample_videos.sh 1920 1080 15;\n   cd ..;\n</code></pre></p> <p>Note</p> <p>Only AVC encoded files are supported.</p> </li> <li> <p>After the video files are downloaded to the sample-media folder, start the camera simulator:     <pre><code>make run-camera-simulator\n</code></pre></p> </li> <li> <p>Wait for few seconds, and then check if the camera-simulator containers are running:     <pre><code>docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}'\n</code></pre></p> <p>Success</p> <p>Your output is as follows:</p> IMAGE STATUS NAMES openvino/ubuntu20_data_runtime:2021.4.2 Up 11 seconds camera-simulator0 aler9/rtsp-simple-server Up 13 seconds camera-simulator <p>Note</p> <p>There could be multiple containers with the image \"openvino/ubuntu20_data_runtime:2021.4.2\", depending on the number of sample-media video files provided.</p> <p>Failure</p> <p>If all the Docker* containers are not visible, then review the console output for errors. Sometimes dependencies fail to resolve. Address obvious issues and retry.</p> </li> </ol>"},{"location":"webcam_rtsp.html","title":"Webcam to RTSP","text":"<p>When using /dev/video0 as input, only one container can use the webcam at the time. If the intention is to run multiple pipelines at once using a webcam as the input, then, the solution is to covert the webcam to an RTSP path.</p> <p>Run:</p> <pre><code>make webcam-rtsp\n</code></pre> <p>Then use the path rtsp://127.0.0.1:8554/cam as input</p>"},{"location":"OVMS/capiPipelineRun.html","title":"OpenVINO OVMS C-API Pipeline Run","text":"<p>OpenVINO Model Server has many ways to run inferencing pipeline: TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here is a demonstration for using OVMS C API method to run face detection inferencing pipeline with steps below:</p> <ol> <li>Add new section to model configuration file for model server</li> <li>Add pipeline specific files</li> <li>Add environment variable file dependency</li> <li>Add a profile launcher pipeline configuration file</li> <li>Build and run</li> </ol>"},{"location":"OVMS/capiPipelineRun.html#add-new-section-to-model-config-file-for-model-server","title":"Add New Section To Model Config File for Model Server","text":"<p>Here is the template config file location: <code>configs/opencv-ovms/models/2022/config_template.json</code>, edit the file and append the following face detection model configuration section to the template <pre><code>,\n{\"config\": {\n      \"name\": \"face-detection-retail-0005\",\n      \"base_path\": \"face-detection-retail-0005/FP16-INT8\",\n      \"shape\": \"(1,3,800,800)\",\n      \"nireq\": 2,\n      \"batch_size\":\"1\",\n      \"plugin_config\": {\"PERFORMANCE_HINT\": \"LATENCY\"},\n      \"target_device\": \"{target_device}\"},\n      \"latest\": { \"num_versions\": 2 }\n    }\n</code></pre></p> <p>Note</p> <p><code>shape</code> is optional and takes precedence over batch_size, please remove this attribute if you don't know the value for the model.</p> <p>Note</p> <p>Please leave <code>target_device</code> value as it is, as the value <code>{target_device}</code> will be recognized and replaced by script run.</p> <p>You can find the parameter description in the ovms docs.</p>"},{"location":"OVMS/capiPipelineRun.html#add-pipeline-specific-files","title":"Add pipeline specific files","text":"<p>Here is the list of files we added in directory of <code>configs/opencv-ovms/gst_capi/pipelines/face_detection/</code>:</p> <ol> <li><code>main.cpp</code> - this is all the work about pre-processing before sending to OVMS for inferencing and post-processing for displaying.</li> <li><code>Makefile</code> - to help building the pre-processing and post-processing binary.</li> </ol>"},{"location":"OVMS/capiPipelineRun.html#add-environment-variable-file","title":"Add Environment Variable File","text":"<p>You can add multiple environment variable files to <code>configs/opencv-ovms/envs/</code> directory for your pipeline. For face detection pipeline run, we have added <code>configs/opencv-ovms/envs/capi_face_detection.env</code> environment variable file. Below is a list of explanation for all environment variables and current default values we set for face detection pipeline run, this list can be extended for any future modification.</p> EV Name Face Detection Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU &amp; GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/face_detection/face_detection pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.9 detection threshold value in floating point that needs to be between 0.0 to 1.0 <p>details of face detection pipeline environment variable file can be viewed in <code>configs/opencv-ovms/envs/capi_face_detection.env</code>.</p>"},{"location":"OVMS/capiPipelineRun.html#add-a-profile-launcher-configuration-file","title":"Add A Profile Launcher Configuration File","text":"<p>The details about Profile Launcher configuration can be found here, details of capi face detection profile launcher configuration can be viewed in <code>configs/opencv-ovms/cmd_client/res/capi_face_detection/configuration.yaml</code>.</p>"},{"location":"OVMS/capiPipelineRun.html#build-and-run","title":"Build and Run","text":"<p>Here are the quick start steps to build and run OVMS C API face detection pipeline profile:</p> <ol> <li>Build gst-capi ovms with profile-launcher: <code>make build-capi_face_detection</code></li> <li>Download sample video files: <code>cd benchmark-scripts/ &amp;&amp; ./download_sample_videos.sh &amp;&amp; cd ..</code></li> <li>Start simulator camera: <code>make run-camera-simulator</code></li> <li>To start face detection pipeline: <code>PIPELINE_PROFILE=\"capi_face_detection\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 --workload ovms</code></li> </ol> <p>Note</p> <p>The pipeline run will automatically download the OpenVINO model files listed in <code>configs/opencv-ovms/models/2022/config_template.json</code></p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html","title":"OpenVINO OVMS C-API Yolov5 Ensemble Pipeline Run","text":"<p>OpenVINO Model Server has many ways to run inferencing pipeline: TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here we are demonstrating for using OVMS C API method to run inferencing pipeline yolov5s ensemble models in following steps:</p> <ol> <li>Add new section to model configuration file for model server</li> <li>Add pipeline specific files</li> <li>Add environment variable file dependency</li> <li>Add a profile launcher pipeline configuration file</li> <li>Build and run</li> <li>Clean up</li> </ol>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-new-section-to-model-config-file-for-model-server","title":"Add New Section To Model Config File for Model Server","text":"<p>The model template configuration file has been updated with model configs of yolov5, efficientnetb0_FP32INT8 and custom configurations, please view <code>configs/opencv-ovms/models/2022/config_template.json</code> for detail.</p> <p>Note</p> <p>New model yolov5 is similar to yolov5s configuration except the layout difference.</p> <p>Note</p> <p>The model efficientnetb0_FP32INT8 is different model from efficientnet-b0.</p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-pipeline-specific-files","title":"Add pipeline specific files","text":"<p>The pre-processing and post-processing work files are added in directory of <code>configs/opencv-ovms/gst_capi/pipelines/capi_yolov5_ensemble/</code>, please view directory for details.</p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-environment-variable-file","title":"Add Environment Variable File","text":"<p>You can add multiple environment variable files to <code>configs/opencv-ovms/envs/</code> directory for your pipeline, we've added <code>capi_yolov5_ensemble.env</code> for yolov5 ensemble pipeline run. Below is a list of explanation for all environment variables and current default values we set, this list can be extended for any future modification.</p> EV Name Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU &amp; GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/capi_yolov5_ensemble/capi_yolov5_ensemble pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.7 detection threshold value in floating point that needs to be between 0.0 to 1.0 <p>details of yolov5s pipeline environment variable file can be viewed in <code>configs/opencv-ovms/envs/capi_yolov5_ensemble.env</code>.</p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#add-a-profile-launcher-configuration-file","title":"Add A Profile Launcher Configuration File","text":"<p>The details about Profile Launcher configuration can be found here, details for yolov5 pipeline profile launcher configuration can be viewed in <code>configs/opencv-ovms/cmd_client/res/capi_yolov5_ensemble/configuration.yaml</code></p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#build-and-run","title":"Build and Run","text":"<p>Here are the quick start steps to build and run capi yolov5 pipeline profile :</p> <ol> <li>Build docker image with profile-launcher: <code>make build-capi_yolov5_ensemble</code></li> <li>Download sample video files: <code>cd benchmark-scripts/ &amp;&amp; ./download_sample_videos.sh &amp;&amp; cd ..</code></li> <li>Start simulator camera: <code>make run-camera-simulator</code></li> <li>To start the pipeline run: <code>PIPELINE_PROFILE=\"capi_yolov5_ensemble\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 --workload ovms</code></li> </ol> <p>Note</p> <p>The pipeline will automatically download the OpenVINO model files listed in <code>configs/opencv-ovms/models/2022/config_template.json</code></p>"},{"location":"OVMS/capiYolov5EnsemblePipelineRun.html#clean-up","title":"Clean Up","text":"<p>To stop existing container: <code>make clean-capi_yolov5_ensemble</code> To stop all running containers including camera simulator and remove all log files: <code>make clean-all</code></p>"},{"location":"OVMS/capiYolov5PipelineRun.html","title":"OpenVINO OVMS C-API Yolov5 Pipeline Run","text":"<p>OpenVINO Model Server has many ways to run inferencing pipeline: TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). Here we are demonstrating for using OVMS C API method to run inferencing pipeline yolov5s model in following steps:</p> <ol> <li>Add new section to model configuration file for model server</li> <li>Add pipeline specific files</li> <li>Add environment variable file dependency</li> <li>Add a profile launcher pipeline configuration file</li> <li>Build and run</li> </ol>"},{"location":"OVMS/capiYolov5PipelineRun.html#add-new-section-to-model-config-file-for-model-server","title":"Add New Section To Model Config File for Model Server","text":"<p>Here is the template config file location: <code>configs/opencv-ovms/models/2022/config_template.json</code>, edit the file and append the new model's configuration into the template, such as yolov5 model as shown below: <pre><code>    {\n      \"config\": {\n        \"name\": \"yolov5s\",\n        \"base_path\": \"/models/yolov5s/FP16-INT8\",\n        \"layout\": \"NHWC:NCHW\",\n        \"shape\": \"(1,416,416,3)\",\n        \"nireq\": 1,\n        \"batch_size\": \"1\",\n        \"plugin_config\": {\n          \"PERFORMANCE_HINT\": \"LATENCY\"\n        },\n        \"target_device\": \"{target_device}\"\n      }\n    }\n</code></pre></p> <p>Note</p> <p><code>shape</code> is optional and takes precedence over batch_size, please remove this attribute if you don't know the value for the model.</p> <p>Note</p> <p>Please leave <code>target_device</code> value as it is, as the value <code>{target_device}</code> will be recognized and replaced by script run.</p> <p>You can find the parameter description in the ovms docs.</p>"},{"location":"OVMS/capiYolov5PipelineRun.html#add-pipeline-specific-files","title":"Add pipeline specific files","text":"<p>Here is the list of files we added in directory of <code>configs/opencv-ovms/gst_capi/pipelines/capi_yolov5/</code>:</p> <ol> <li><code>main.cpp</code> - this is all the work about pre-processing before sending to OVMS for inferencing and post-processing for displaying.</li> <li><code>Makefile</code> - to help building the pre-processing and post-processing binary.</li> </ol>"},{"location":"OVMS/capiYolov5PipelineRun.html#add-environment-variable-file","title":"Add Environment Variable File","text":"<p>You can add multiple environment variable files to <code>configs/opencv-ovms/envs/</code> directory for your pipeline, we've added <code>capi_yolov5.env</code> for yolov5 pipeline run. Below is a list of explanation for all environment variables and current default values we set, this list can be extended for any future modification.</p> EV Name Default Value Description RENDER_PORTRAIT_MODE 1 rendering in portrait mode, value: 0 or 1 GST_DEBUG 1 running GStreamer in debug mode, value: 0 or 1 USE_ONEVPL 1 using OneVPL CPU &amp; GPU Support, value: 0 or 1 PIPELINE_EXEC_PATH pipelines/capi_yolov5/capi_yolov5 pipeline execution path inside container GST_VAAPI_DRM_DEVICE /dev/dri/renderD128 GStreamer VAAPI DRM device input TARGET_GPU_DEVICE --privileged allow using GPU devices if any LOG_LEVEL 0 GST_DEBUG log level to be set when running gst pipeline RENDER_MODE 1 option to display the input source video stream with the inferencing results, value: 0 or 1 cl_cache_dir /home/intel/gst-ovms/.cl-cache cache directory in container WINDOW_WIDTH 1920 display window width WINDOW_HEIGHT 1080 display window height DETECTION_THRESHOLD 0.7 detection threshold value in floating point that needs to be between 0.0 to 1.0 <p>details of yolov5s pipeline environment variable file can be viewed in <code>configs/opencv-ovms/envs/capi_yolov5.env</code>.</p>"},{"location":"OVMS/capiYolov5PipelineRun.html#add-a-profile-launcher-configuration-file","title":"Add A Profile Launcher Configuration File","text":"<p>The details about Profile Launcher configuration can be found here, details for yolov5 pipeline profile launcher configuration can be viewed in <code>configs/opencv-ovms/cmd_client/res/capi_yolov5/configuration.yaml</code></p>"},{"location":"OVMS/capiYolov5PipelineRun.html#build-and-run","title":"Build and Run","text":"<p>Here are the quick start steps to build and run capi yolov5 pipeline profile :</p> <ol> <li>Build docker image with profile-launcher: <code>make build-capi_yolov5</code></li> <li>Download sample video files: <code>cd benchmark-scripts/ &amp;&amp; ./download_sample_videos.sh &amp;&amp; cd ..</code></li> <li>Start simulator camera: <code>make run-camera-simulator</code></li> <li>To start the pipeline run: <code>PIPELINE_PROFILE=\"capi_yolov5\" RENDER_MODE=1 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_1 --workload ovms</code></li> </ol> <p>Note</p> <p>The pipeline will automatically download the OpenVINO model files listed in <code>configs/opencv-ovms/models/2022/config_template.json</code></p>"},{"location":"OVMS/pipelineDockerCompose.html","title":"Docker-Compose for Developer Toolbox","text":"<p>Pipelines can be run using docker-compose files. Changes are custom made inside the <code>docker-compose.yml</code> file for integration with the Developer Toolbox.</p> <p>Note</p> <p>To utilize all the features offered by Automated Self-Checkout, run the pipelines as illustrated in the section Run Pipelines.</p>"},{"location":"OVMS/pipelineDockerCompose.html#steps-to-run-pipelines","title":"Steps to Run Pipelines","text":"<ol> <li> <p>Prerequisites</p> <p>Before running, Set Up the Pipelines.</p> <p>Note</p> <p>Ensure Docker Compose v2 is installed in order to run the pipelines via this feature. </p> </li> <li> <p>Customize the <code>docker-compose.yml</code> to add the number of camera simulators required and the number of different type of pipelines that need to be run</p> <p>Note</p> <p>Follow all the instructions in <code>docker-compose.yml</code> for customizations.</p> </li> <li> <p>Run the pipelines</p> <pre><code>make run-pipelines\n</code></pre> </li> <li> <p>All the containers i.e camera simulators, OVMS server and pipelines should start without any errors in portainer as shown below in Figure 1 -</p> <p> Figure 1: Pipelines Running Successfully </p> </li> <li> <p>Stop the pipelines</p> <pre><code>make down-pipelines\n</code></pre> </li> </ol>"},{"location":"OVMS/pipelinerun.html","title":"Customize Run","text":""},{"location":"OVMS/pipelinerun.html#overview","title":"Overview","text":"<p>When the pipeline is run, the <code>run.sh</code> script starts the service and performs inferencing on the selected input media. The output of running the pipeline provides the inference results for each frame based on the media source such as text, barcode, and so on, as well as the frames per second (FPS). Pipeline run provides many options in media type, system process platform type, and additional optional parameters. These options give you the opportunity to compare what system process platform is better for your need.</p>"},{"location":"OVMS/pipelinerun.html#start-pipeline","title":"Start Pipeline","text":"<p>You can run the pipeline script, <code>run.sh</code> with <code>--workload ovms</code> option, and the following additional input parameters:</p> <ol> <li>Media type<ul> <li>Camera Simulator using RTSF</li> <li>USB Camera</li> <li>Video File</li> </ul> </li> <li>Platform<ul> <li>core</li> <li>dgpu.0</li> <li>dgpu.1</li> <li>xeon</li> </ul> </li> <li>Optional parameters</li> <li>Environment Variables</li> </ol> <p>Run the command based on your requirement. You have to get your choices for #1-4 above to start the pipeline run, see details section below.</p>"},{"location":"OVMS/pipelinerun.html#optional-parameters","title":"Optional Parameters","text":"<p>The following are the optional parameters that you can provide as input to <code>run.sh</code>. Note that these parameters would affect the performance of the pipeline.</p> <ul> <li> <p><code>--ocr</code>: Provides the OCR frame internal value, such as <code>--ocr 5 GPU</code>. The default recognition interval value is 5. Note device equal to CPU is not supported when executing with a discrete GPU.</p> </li> <li> <p><code>--barcode_disabled</code>: Disables barcode detection. By default, barcode detection is enabled.</p> </li> <li> <p><code>--color-width</code>, <code>color-height</code>, and <code>color-framerate</code>: Allows you to customize the settings of the color frame output from the Intel\u00ae RealSense\u2122 Cameras. This parameter will overwrite the default value of RealSense gstreamer. Use <code>rs-enumerate-devices</code> to look up the camera's color capability.</p> </li> </ul>"},{"location":"OVMS/pipelinerun.html#environment-variables","title":"Environment variables","text":"<p>When running run.sh script, we support environment variables as input for containers. Here is a list of environment variables and how to apply them</p> <p>Here is an example how to apply environment variables when running pipeline using <code>ovms</code> workload: <pre><code>PIPELINE_PROFILE=\"instance_segmentation\" RENDER_MODE=1 sudo -E ./run.sh --workload ovms --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre></p>"},{"location":"OVMS/pipelinerun.html#run-pipeline-with-different-input-sourceinputsrc-types","title":"Run pipeline with different input source(inputsrc) types","text":"<p>Use run.sh to run the pipeline, here is the table of basic scripts for each combination:</p> Input source Type Command Simulated camera <code>sudo ./run.sh --workload ovms --platform core|xeon|dgpu.x --inputsrc rtsp://127.0.0.1:8554/camera_0</code> RealSense camera <code>sudo ./run.sh --workload ovms --platform core|xeon|dgpu.x --inputsrc  --realsense_enabled USB camera <code>sudo ./run.sh --workload ovms --platform core|xeon|dgpu.x --inputsrc /dev/video0</code> Video file <code>sudo ./run.sh --workload ovms --platform core|xeon|dgpu.x --inputsrc file:my_video_file.mp4</code> <p>Note</p> <p>For simulated camera as input source, please run camera simulator first.</p> <p>Note</p> <p>The value of x in <code>dgpu.x</code> can be 0, 1, 2, and so on depending on the number of discrete GPUs in the system.</p> <p>Note</p> <p>Follow these steps to see the output formats supported by your USB camera.</p>"},{"location":"OVMS/pipelinerun.html#supporting-different-programming-languages-for-ovms-grpc-client","title":"Supporting different programming languages for OVMS grpc client","text":"<p>We are supporting multiple programming languages for OVMS grpc client. Currently we are supporting grpc-python and grpc-go. The scripts to start pipelines above would start grpc-python as default. See more on supporting different language</p>"},{"location":"OVMS/pipelinerun.html#supporting-different-models-for-ovms-grpc-python-client","title":"Supporting different models for OVMS grpc python client","text":"<p>With OVMS grpc-python client, you can configure to use different model to run the inferencing pipeline. The scripts to start pipelines above would start grpc-python using <code>instance-segmentation-security-1040</code> model as default. See more on supporting different model</p>"},{"location":"OVMS/pipelinerun.html#stop-pipeline-run","title":"Stop pipeline run","text":"<p>You can call <code>make clean-ovms</code> to stop the pipeline and all running containers for ovms, hence the results directory log files will stop growing. Below is the table of make commands you can call to clean things up per your needs:</p> Clean Containers Options Command clean instance-segmentation container if any <pre>make clean-segmentation</pre> clean grpc-go dev container if any <pre>make clean-grpc-go</pre> clean all related containers launched by profile-launcher if any <pre>make clean-profile-launcher</pre> clean ovms-server container <pre>make clean-ovms-server</pre> clean ovms-server and all containers launched by profile-launcher <pre>make clean-ovms</pre> clean results/ folder <pre>make clean-results</pre>"},{"location":"OVMS/pipelinesetup.html","title":"Set up Pipeline","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone  https://github.com/intel-retail/automated-self-checkout.git &amp;&amp; cd ./automated-self-checkout\n</code></pre> </li> <li> <p>Build the profile launcher binary executable</p> <pre><code>make build-profile-launcher\n</code></pre> <p>Each profile is an unique pipeline use case.  We provide some profile examples, and the configuration examples of profiles are located here.  Go here to find out the detail descriptions for the configuration of profile used by profile launcher.</p> </li> <li> <p>Build the benchmark Docker images</p> <pre><code>cd benchmark-scripts\nmake build-all\n\ncd ..\n</code></pre> <p>Note</p> <p>A successfully built benchmark Docker images should contain the following Docker images from <code>docker images</code> command:</p> <ul> <li>benchmark:dev</li> <li>benchmark:xpu</li> <li>benchmark:igt</li> </ul> <p>Note</p> <p>After successfully built benchmark Docker images, please remember to change the directory back to the project base directory from the current benchmark-scripts directory (i.e. <code>cd ..</code>) for the following steps.        </p> </li> <li> <p>Download the models manually (Optional)</p> <p>Note</p> <p>The model downloader script is automatically called as part of run-ovms.sh (part of run.sh).</p> <pre><code>./download_models/getModels.sh --workload ovms\n</code></pre> <p>Warning</p> <p>Depending on your internet connection, this might take some time.</p> </li> <li> <p>(Optional) Download the video file manually. This video is used as the input source to give to the pipeline.</p> <p>Note</p> <p>The sample image downloader script is automatically called as part of run-ovms.sh. </p> <pre><code>./configs/opencv-ovms/scripts/image_download.sh\n</code></pre> <p>Warning</p> <p>Depending on your internet connection, this might take some time.</p> </li> <li> <p>(optional) Download the bit model manually </p> <p>a. Here is the command to build the container for bit model downloading:</p> <pre><code>```bash\ndocker build -f Dockerfile.bitModel -t bit_model_downloader:dev .\n```\n</code></pre> <p>b. Here is the script to run the container that downloads the bit models:</p> <pre><code>```bash\ndocker run -it bit_model_downloader:dev\n```\n</code></pre> </li> <li> <p>Build the reference design images. This table shows the commands for the OpenVINO (OVMS) model Server and profile-launcher build command:</p> Target Docker Build Command Check Success OVMS Server <pre>make build-ovms-server</pre> <code>docker images</code> command output contains Docker image openvino/model_server:2023.1-gpu OVMS Profile Launcher <pre>make build-profile-launcher</pre> <code>ls -al ./profile-launcher</code> command to show the binary executable <p>Note</p> <p>Build command may take a while, depending on your internet connection and machine specifications.</p> <p>Note</p> <p>If the build command succeeds, you will see all the built Docker images files as indicated in the Check Success column. If the build fails, check the console output for errors.</p> <p>Proxy</p> <p>If docker build system requires a proxy network, just set your proxy env standard way on your terminal as below and make build:</p> <pre><code>export HTTP_PROXY=\"http://your-proxy-url.com:port\"\nexport HTTPS_PROXY=\"https://your-proxy-url.com:port\"\nmake build-ovms-server\nmake build-profile-launcher\n</code></pre> </li> </ol>"},{"location":"OVMS/profileLauncherConfigs.html","title":"Profile Configuration","text":"<p>For the profile launcher, each profile has its own configuration for different pipelines.  The configuration of each profile is done through a yaml configuration file, configuration.yaml.  One example of configuration.yaml is shown here for classification profile:</p> <pre><code>OvmsSingleContainer: false\nOvmsServer:\n  ServerDockerScript: start_ovms_server.sh\n  ServerDockerImage: openvino/model_server:2023.1-gpu\n  ServerContainerName: ovms-server\n  ServerConfig: \"/models/config.json\"\n  StartupMessage: Starting OVMS server\n  InitWaitTime: 10s\n  EnvironmentVariableFiles:\n    - ovms_server.env\n  # StartUpPolicy:\n  # when there is an error on launching ovms server startup, choose one of these values for the behavior of profile-launcher: \n  #   remove-and-restart: it will remove the existing container with the same container name if any and then restart the container\n  #   exit: it will exit the profile-launcher and \n  #   ignore: it will ignore the error and continue (this is the default value if not given or none of the above)\n  StartUpPolicy: ignore    \nOvmsClient:\n  DockerLauncher:\n    Script: docker-launcher.sh\n    DockerImage: python-demo:dev\n    ContainerName: classification\n    Volumes:\n      - \"$RUN_PATH/results:/tmp/results\"\n      - ~/.Xauthority:/home/dlstreamer/.Xauthority\n      - /tmp/.X11-unix\n  PipelineScript: ./classification/python/entrypoint.sh\n  PipelineInputArgs: \"\" # space delimited like we run the script in command and take those input arguments\n  EnvironmentVariableFiles:\n    - classification.env\n</code></pre> <p>The description of each configuration element is explained below:</p> Configuration Element Description OvmsSingleContainer This boolean flag indicates whether this profile is running as a single OpenVino Model Server (OVMS) container or not, e.g. the C-API pipeline use case will use this as <code>true</code>. It can indicate the distributed architecture of OVMS client-server when this flag is false.  OvmsServer This is configuration section for OpenVino Model Server in the case of client-server architecture. OvmsServer/ServerDockerScript The infra-structure shell script to start an instance of OVMS server. OvmsServer/ServerDockerImage The Docker image tag name for OpenVino Model Server. OvmsServer/ServerContainerName The Docker container base name for OpenVino Model Server. OvmsServer/ServerConfig The model config.json file name path for OpenVino Model Server. OvmsServer/StartupMessage The starting message shown in the console or log when OpenVino Model Server instance is launched. OvmsServer/InitWaitTime The waiting time duration (like 5s, 5m, .. etc) after OpenVino Model Server is launched to allow some settling time before launching the pipeline from the client. OvmsServer/EnvironmentVariableFiles The list of environment variable files applied for starting OpenVino Model Server Docker instance. OvmsServer/StartUpPolicy This configuration controls the behavior of OpenVino Model Server Docker instance when there is error occurred during launching. Use one of these values: <code>remove-and-restart</code>: it will remove the existing container with the same container name if any and then restart the container  <code>exit</code>: it will exit the profile-launcher <code>ignore</code>: it will ignore the error and continue (this is the default value if not given or none of the above).  OvmsClient This is configuration section for the OVMS client running pipelines in the case of client-server architecture. The C-API pipeline use case should also use this section to configure. OvmsClient/DockerLauncher This is configuration section for the generic Docker launcher to run pipelines for a given profile. OvmsClient/DockerLauncher/Script The generic Docker launcher script file name. OvmsClient/DockerLauncher/DockerImage The Docker image tag name for the pipeline profile. OvmsClient/DockerLauncher/ContainerName The Docker container base name for the running pipeline profile. OvmsClient/DockerLauncher/Volumes The Docker container volume mounts for the running pipeline profile. OvmsClient/PipelineScript The file name path for the pipeline profile to launch. The file path here is in the perspective of the running container. i.e. the path inside the running container. OvmsClient/PipelineInputArgs Any input arguments or parameters for the above pipeline script to take. Like any command line argument, they are space-delimited if multiple arguments. OvmsClient/EnvironmentVariableFiles The list of environment variable files applied for the running pipeline profile Docker instance."},{"location":"OVMS/quick_pipelinerun.html","title":"Quick Start Guide to Run Pipeline","text":""},{"location":"OVMS/quick_pipelinerun.html#prerequisites","title":"Prerequisites","text":"<p>Before running, set up the pipeline.</p>"},{"location":"OVMS/quick_pipelinerun.html#running-ovms-with-the-camera-simulator","title":"Running OVMS with the camera simulator","text":""},{"location":"OVMS/quick_pipelinerun.html#start-the-camera-simulator","title":"Start the Camera Simulator","text":"<ol> <li> <p>Download the video files to the sample-media directory:     <pre><code>cd benchmark-scripts;\n./download_sample_videos.sh;\ncd ..;\n</code></pre></p> <p>Example - Specify Resolution and Framerate</p> <p>This example downloads a sample video for 1080p@15fps. <pre><code>cd benchmark-scripts;\n./download_sample_videos.sh 1920 1080 15;\ncd ..;\n</code></pre></p> <p>Note</p> <p>Only AVC encoded files are supported.</p> </li> <li> <p>After the video files are downloaded to the sample-media folder, start the camera simulator:     <pre><code>make run-camera-simulator\n</code></pre></p> </li> <li> <p>Wait for few seconds, and then check if the camera-simulator containers are running:     <pre><code>docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}'\n</code></pre></p> <p>Success</p> <p>Your output is as follows:</p> IMAGE STATUS NAMES openvino/ubuntu20_data_runtime:2021.4.2 Up 11 seconds camera-simulator0 aler9/rtsp-simple-server Up 13 seconds camera-simulator <p>Note</p> <p>There could be multiple containers with the image \"openvino/ubuntu20_data_runtime:2021.4.2\", depending on the number of sample-media video files provided.</p> <p>Failure</p> <p>If all the Docker* containers are not visible, then review the console output for errors. Sometimes dependencies fail to resolve. Address obvious issues and retry.</p> </li> </ol>"},{"location":"OVMS/quick_pipelinerun.html#run-instance-segmentation","title":"Run Instance Segmentation","text":"<p>There are several pipeline profiles to chose from. Use the <code>make list-profiles</code> to see the different pipeline options. In this example, the <code>instance_segmentation</code> pipeline profile will be used. </p> <ol> <li> <p>Use the following command to run instance segmentation using OVMS on core.</p> <pre><code>PIPELINE_PROFILE=\"instance_segmentation\" RENDER_MODE=1 sudo -E ./run.sh --workload ovms --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre> </li> <li> <p>Check the status of the pipeline.</p> <pre><code>docker ps --format 'table{{.Image}}\\t{{.Status}}\\t{{.Names}}' -a\n</code></pre> <p>Success</p> <p>Here is a sample output:</p> IMAGE STATUS NAMES openvino/model_server-gpu:latest Up 59 seconds ovms-server0 <p>Failure</p> <p>If you do not see above Docker container(s), review the console output for errors. Sometimes dependencies fail to resolve and must be run again. Address obvious issues and try again repeating the above steps. Here are couple debugging tips:</p> <ol> <li> <p>check the docker logs using following command to see if there is an issue with the container</p> <pre><code>docker logs &lt;containerName&gt;\n</code></pre> </li> <li> <p>check ovms log in automated-self-checkout/results/r0.jsonl</p> </li> </ol> </li> <li> <p>Check the output in the <code>results</code> directory.</p> <p>Example - results/r0.jsonl sample</p> <p>The output in results/r0.jsonl file lists average processing time in milliseconds and average number of frames per second. This file is intended for scripts to parse.  <pre><code>Processing time: 53.17 ms; fps: 18.81\nProcessing time: 47.98 ms; fps: 20.84\nProcessing time: 48.35 ms; fps: 20.68\nProcessing time: 46.88 ms; fps: 21.33\nProcessing time: 47.56 ms; fps: 21.03\nProcessing time: 49.66 ms; fps: 20.14\nProcessing time: 52.49 ms; fps: 19.05\nProcessing time: 52.27 ms; fps: 19.13\nProcessing time: 50.86 ms; fps: 19.66\nProcessing time: 58.19 ms; fps: 17.18\nProcessing time: 58.28 ms; fps: 17.16\nProcessing time: 52.17 ms; fps: 19.17\nProcessing time: 50.89 ms; fps: 19.65\nProcessing time: 49.58 ms; fps: 20.17\nProcessing time: 51.14 ms; fps: 19.55\n</code></pre></p> <p>Example - results/pipeline0.log sample</p> <p>The output in results/pipeline0.log lists average number of frames per second. Below is a snap shot of the output: <pre><code>18.81\n20.84\n20.68\n21.33\n21.03\n20.14\n19.05\n19.13\n19.66\n17.18\n17.16\n19.17\n19.65\n20.17\n19.55\n</code></pre></p> <p>Note</p> <p>The automated-self-checkout/results/ directory is volume mounted to the pipeline container.</p> </li> </ol>"},{"location":"OVMS/quick_pipelinerun.html#stop-running-the-pipelines","title":"Stop running the pipelines","text":"<ol> <li>To stop the instance segmentation container and clean up, run      <pre><code>make clean-all\n</code></pre></li> </ol>"},{"location":"OVMS/quick_stream_density.html","title":"Quick Start Guide to Run Pipeline Stream Density","text":"<p>In this section, we show the steps to run the stream density for a chosen pipeline profile.  By definition, the objective of the stream density is to bench-mark the maximum number of multiple running pipelines at the same time while still maintaining the goal-setting target frames-per-second (FPS).</p>"},{"location":"OVMS/quick_stream_density.html#prerequisites","title":"Prerequisites","text":"<p>Before running, set up the pipeline if not already done.</p>"},{"location":"OVMS/quick_stream_density.html#stop-all-other-running-pipelines","title":"Stop All Other Running Pipelines","text":"<p>To make sure we have a good stream density benchmarking, it is recommended to stop all other running pipelines before running the stream density. To stop all running pipelines and clean up, run     <pre><code>make clean-all\n</code></pre></p>"},{"location":"OVMS/quick_stream_density.html#build-benchmark-docker-images","title":"Build Benchmark Docker Images","text":"<p>For running stream density, the benchmark scripts are utilized.  To set up the benchmarking, we need to build the benchmark Docker images first.</p> <ol> <li> <p>Build the benchmark Docker* images     Benchmark scripts are containerized inside Docker. The easiest way to build all benchmark Docker images, run         <pre><code>cd ./benchmark-scripts\nmake\n</code></pre></p> <p>It is also possible to choose which benchmark Docker images to build based on different platforms.</p> <p>The following table lists the commands for various platforms. Choose and run the command corresponding to your hardware configuration.</p> Platform Docker Build Command Check Success Intel\u00ae integrated and Arc\u2122 GPUs <pre>cd benchmark-scriptsmake build-benchmarkmake build-igt</pre> Docker images command to show both <code>benchmark:dev</code> and <code>benchmark:igt</code> images Intel\u00ae Flex GPUs <pre>cd benchmark-scriptsmake build-benchmarkmake build-xpu</pre> Docker images command to show both <code>benchmark:dev</code> and <code>benchmark:xpu</code> images <p>Warning</p> <p>Build command may take a while, depending on your internet connection and machine specifications.</p> </li> </ol>"},{"location":"OVMS/quick_stream_density.html#start-the-camera-simulator","title":"Start the Camera Simulator","text":"<p>We will use the camera simulator as the input source to show the stream density. Please refer to the section of Start the Camera Simulator in Quick Start Guide to Run Pipeline on how to start the camera simulator.</p>"},{"location":"OVMS/quick_stream_density.html#run-objection-detection-pipeline-stream-density","title":"Run Objection Detection Pipeline Stream Density","text":"<p>There are several pipeline profiles to choose from for running pipeline stream density. Use the <code>make list-profiles</code> to see the different pipeline options. In this example, the <code>object_detection</code> pipeline profile will be used for running stream density.</p> <ol> <li> <p>To run the stream density, the benchmark shell script, <code>benchmark.sh</code>, is used. The script is in the &lt;project_base_dir&gt;/benchmark-scripts directory.  Use the following command to run objection detection pipeline profile using OVMS on core.</p> <pre><code>cd ./benchmark-scripts\nPIPELINE_PROFILE=\"object_detection\" RENDER_MODE=0 sudo -E ./benchmark.sh --stream_density 15.0 --logdir object_detection/data --duration 120 --init_duration 40 --workload ovms --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0\n</code></pre> <p>Note</p> <p>Description of some key benchmarking input parameters is given as below:</p> Parameter Name Example Value Description --stream_density 15.0 The value 15.0 after the --stream_density is the target FPS that we want to achieve for running maximum number of object detection pipelines while the averaged of all pipelines from the output FPS still maintaining that target FPS value. --logdir object_detection/data the output directory of benchmarking resource details --duration 120 the time duration, in number of seconds, the benchmarking will run --init_duration 40 the time duration, in number of seconds, to wait for system initialization before the benchmarking metrics or data collection begins <p>Note</p> <p>For stream density run, it is recommended to turn off the display to conserve the system resources hence setting <code>RENDER_MODE=0</code></p> <p>Note</p> <p>This takes a while for the whole stream density benchmarking process depending on your system resources like CPU, memory, ... etc.</p> <p>Note</p> <p>The benchmark.sh script automatically cleans all running Docker containers after it is done.</p> </li> <li> <p>Check the output in the base <code>results</code> directory.</p> <p>After the stream density is done, the results of stream density can be seen on the base directory of the <code>results</code> directory:</p> <pre><code>cat ../results/stream_density.log\n</code></pre> <p>Example - results/stream_density.log sample</p> <p>The output in results/stream_density.log file gives the detailed information of stream density results:  <pre><code>   ......\n   FPS for pipeline0: 15.1225\n   FPS for pipeline1: 15.19\n   FPS for pipeline2: 15.18\n   Total FPS throughput: 45.4925\n   Total FPS per stream: 15.1642\n   Max stream density achieved for target FPS 15.0 is 3\n   Finished stream density benchmarking\n   stream_density done!\n</code></pre></p> </li> </ol>"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html","title":"Run Object Detection Pipeline with New Model","text":"<p>OpenVINO Model Server has many ways to run inferencing pipeline: TensorFlow Serving gRPC API, KServe gRPC API, TensorFlow Serving REST API, KServe REST API and OVMS C API through OpenVINO model server (OVMS). For running object detection pipeline, it is based on KServe gRPC API method, default model used is ssd_mobilenet_v1_coco. You can use different model to run object detection. Here are the steps:</p> <ol> <li>Add new section to config file for model server</li> <li>Download new model</li> <li>Update environment variables of detection pipeline for new model</li> <li>Build and Run</li> </ol>"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#add-new-section-to-config-file-for-model-server","title":"Add New Section to Config File for Model Server","text":"<p>Here is the config file location: <code>configs/opencv-ovms/models/2022/config_template.json</code>, edit the file and append the following configuration section template <pre><code>,\n{\n      \"config\": {\n        \"name\": \"ssd_mobilenet_v1_coco\",\n        \"base_path\": \"/models/ssd_mobilenet_v1_coco/FP32\",\n        \"nireq\": 1,\n        \"batch_size\": \"1\",\n        \"plugin_config\": {\n          \"PERFORMANCE_HINT\": \"LATENCY\"\n        },\n        \"target_device\": \"{target_device}\"\n      },\n      \"latest\": {\n        \"num_versions\": 1\n      }\n    }\n</code></pre></p> <p>Note</p> <p>Please leave <code>target_device</code> value as it is, as the value <code>{target_device}</code> will be recognized and replaced by script run.</p> <p>You can find the parameter description in the ovms docs.</p>"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#download-new-model","title":"Download New Model","text":"<p>The pipeline run script automatically download the model files if it is part of open model zoo supported list; otherwise, please add your model files manually to <code>configs/opencv-ovms/models/2022/</code>. When you add your model manually, make sure to follow the model file structure as //1/modelfiles, for example: <pre><code>ssd_mobilenet_v1_coco\n\u251c\u2500\u2500 FP32\n\u00a0\u00a0 \u2514\u2500\u2500 1\n\u00a0\u00a0     \u251c\u2500\u2500 ssd_mobilenet_v1_coco.bin\n\u00a0\u00a0     \u2514\u2500\u2500 ssd_mobilenet_v1_coco.xml\n</code></pre>"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#update-environment-variables","title":"Update Environment Variables","text":"<p>You can update the object detection environment variables in file: <code>configs/opencv-ovms/envs/object_detection.env</code>, here is default value and explanation for each environment variable:</p> EV Name Default Value Description DETECTION_MODEL_NAME ssd_mobilenet_v1_coco model name for object detection DETECTION_LABEL_FILE coco_91cl_bkgr.txt label file name to use on object detection for model DETECTION_ARCHITECTURE_TYPE ssd architecture type for object detection model DETECTION_OUTPUT_RESOLUTION 1920x1080 output resolution for object detection result DETECTION_THRESHOLD 0.50 threshold for object detection in floating point that needs to be between 0.0 to 1.0 MQTT enable MQTT notification of result, value: empty RENDER_MODE 1 display the input source video stream with the inferencing results, value: 0"},{"location":"OVMS/runObjectDetectionPipelineWithNewModel.html#build-and-run-pipeline","title":"Build and Run Pipeline","text":"<ol> <li>Build the python app and profile-launcher: <code>make build-python-apps</code></li> <li>Download sample video files: <code>cd benchmark-scripts/ &amp;&amp; ./download_sample_videos.sh &amp;&amp; cd ..</code></li> <li>Start simulator camera if not started: <code>make run-camera-simulator</code></li> <li>(Optional) Run MQTT broker: <code>docker run --network host --rm -d -it -p 1883:1883 -p 9001:9001 eclipse-mosquitto</code></li> <li>To start object detection pipeline: <code>PIPELINE_PROFILE=\"object_detection\" RENDER_MODE=1 MQTT=127.0.0.1:1883 sudo -E ./run.sh --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0 --workload ovms</code> (remove the MQTT environment variable if not using it)</li> <li>If do use MQTT, use the container name as the MQTT topic to subscribe to the inference metadata. Do a <code>docker ps</code> to know the container name.</li> <li>To stop the running pipelines: <code>make clean-profile-launcher</code> to stop and clean up the client side containers, or <code>make clean-all</code> to stop and clean up everything.</li> </ol>"},{"location":"OVMS/supportingDifferentLanguage.html","title":"Supporting Different Languages","text":"<p>For running OVMS as inferencing engine through grpc, we are supporting multiple programming languages for your need. Here is the list of languages we are supporting:</p> <ol> <li>python</li> <li>golang</li> </ol> <p>Here is the script example to start grpc-python using rtsp as inputsrc: <code>PIPELINE_PROFILE=\"grpc_python\" sudo -E ./run.sh --workload ovms --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0</code></p> <p>Here is the script example to start grpc-go using rtsp as inputsrc: <code>PIPELINE_PROFILE=\"grpc_go\" sudo -E ./run.sh --workload ovms --platform core --inputsrc rtsp://127.0.0.1:8554/camera_0</code></p> <p>Note</p> <p>Above example scripts are based on camera simulator for rtsp input source, before running them, please run camera simulator. If you used pipeline scripts other than rtsp input source, then you don't need to run camera simulator.</p>"},{"location":"OVMS/supportingDifferentModel.html","title":"Support Different Model","text":"<p>For running OVMS as inferencing engine through grpc, we are supporting different models for your need. </p>"},{"location":"OVMS/supportingDifferentModel.html#models-supported-in-python","title":"Models Supported In Python","text":"<p>Here is the list of inferencing models we are currently supporting in python:</p> <ol> <li>instance-segmentation-security-1040</li> <li>bit_64</li> </ol> <p>You can switch between them by editing the configuration file <code>configs/opencv-ovms/cmd_client/res/grpc_python/configuration.yaml</code>, uncomment <code># PipelineInputArgs: \"--model_name instance-segmentation-security-1040\"</code> for supporting instance-segmentation-security-1040 and comment out rest; or you can uncomment <code># PipelineInputArgs: \"--model_name bit_64\"</code> for supporting bit_64 and comment out rest.</p> <p>Here is the configuration.yaml content, default to use <code>instance-segmentation-security-1040</code> model <pre><code>OvmsClient:\n  PipelineScript: run_grpc_python.sh\n  PipelineInputArgs: \"--model_name instance-segmentation-security-1040\" # space delimited like we run the script in command and take those input arguments\n  # PipelineInputArgs: \"--model_name bit_64\" # space delimited like we run the script in command and take those input arguments\n  # PipelineInputArgs: \"--model_name yolov5s\" # space delimited like we run the script in command and take those input arguments\n</code></pre></p>"},{"location":"OVMS/supportingDifferentModel.html#download-models","title":"Download Models","text":"<p>You can download models by editing <code>download_models/models.lst</code> file, you can add new models(from https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/object_detection_demo/python/models.lst) to it or uncomment from existing list in this file, saved the file once editing is done. Then you can download the list using following steps:</p> <ol> <li><code>cd download_models</code></li> <li><code>make build</code></li> <li><code>make run</code></li> </ol> <p>after above steps, the downloaded models can be found in <code>configs/opencv-ovms/models/2022</code> directory.</p> <p>Note</p> <p>Model files in <code>configs/opencv-ovms/models/2022</code> directory will be replaced with new downloads if previously existed.</p>"},{"location":"release-notes/v1-0-1.html","title":"1.0.1","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.0.0 is the first major release. This release includes all items required to run the vision checkout pipeline and benchmarking. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-0-1.html#new-features","title":"New Features","text":"Title Description Pipeline scripts Scripts that run the GStreamer-based vision checkout pipeline Benchmark scripts Scripts that start pipelines and system metrics based on parameters Docker* images Dockerized images for the pipeline and benchmark tools for code portability Set up Documentation Markdown files that include setup steps for initial use Unit tests Basic unit tests scripts for smoke testing Camera simulator Camera simulator script to simulate an RTSP stream using a media file Media downloader script Script to assist with downloading sample media for the camera simulator Model downloader script Script to assist with downloading the model files used for the pipelines"},{"location":"release-notes/v1-0-1.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description Link None Initial Release"},{"location":"release-notes/v1-0-1.html#known-issues","title":"Known Issues","text":"Issue Number Description Link 15 Pipeline core run on some HW is not producing inference results https://github.com/intel-retail/vision-self-checkout/issues/15 29 Unable to modify batch size from run script https://github.com/intel-retail/vision-self-checkout/issues/29"},{"location":"release-notes/v1-5-0.html","title":"1.5.0","text":"<p>Intel\u00ae Automated Self-Checkout Reference Package 1.5.0 is the second main release. This release includes bug fixes, feature enhancements, dockerization of the benchmarking tools, and OpenVINO Model Server support. For details on running the solution, refer to the Overview. </p>"},{"location":"release-notes/v1-5-0.html#new-features","title":"New Features","text":"Title Description OpenVINO Model Server OpenVINO Model Server support OpenVINO Model Server Pipelines Object detection pipelines using OpenVINO Model Server Benchmark scripts Dockerization Benchmark tools have been moved to Docker containers for more flexible deployment Github Build actions Code linting and security scans for pull requests"},{"location":"release-notes/v1-5-0.html#issues-fixed","title":"Issues Fixed","text":"Issue Number Description 41 Pipeline failure log 42 Create makefile docker commands 51 Optimized density script to reduce run time on high powered systems 55 Make performance / powersave mode configurable 57 Add debug option for docker-run.sh 58 Doc update with makefile 61 rename vision self checkout to automated self checkout 65 Update documentation to include OVMS pipelines 66 Add model download top level script 67 [Tech Debt] Make --workload work in any option/argument position when run benchmark.sh 75 docker-run.sh with wrong message when no --workload option is provided 77 XPU Manager not running on multiple GPUs 85 Fix ShellCheck issues in scripts 88 Incorrect instructions for building IGT in pipelinebenchmarking.md 91 format avc mp4 tag logic is inverted 96 For ovms workload getModels.sh not working when it is called by docker-run.sh from project base directory 99 Clean up some checked in dlstreamer models 100 Add cleaning ovms containers to makefile 105 benchmark pcm directory incorrect 109 igt path pointing to the incorrect directory causing the igt log to not be written 112 make CPU as default device for ovms pipeline 115 add dockerfile.bitModel to download bit models 119 pipelinesetup doc has incorrect link to models.list.yml 124 add ovms sample image download into run script 129 Update License to Apache 2.0 131 update mkdoc to navigate to OVMS doc 142 make build-ovms-server failed for 2nd time or later after removed the Docker image for rebuild"},{"location":"release-notes/v1-5-0.html#known-issues","title":"Known Issues","text":"Issue Number Description Link None"},{"location":"telemetry/setup.html","title":"Setup Telemetry","text":"<ol> <li> <p>Build Telegraf docker image</p> <pre><code>make build-telegraf\n</code></pre> </li> <li> <p>Run InfluxDB and Telegraf. Set password for InfluxDB as env variable in command line:</p> <p>Note</p> <p>Password must be at least 8 characters in length.</p> <pre><code>make INFLUXPASS=yourpass run-telegraf\n</code></pre> </li> <li> <p>Start the dashboard by opening a browser to http://127.0.0.1:8086 and login using username: telegraf and the password you set previously.</p> </li> <li> <p>Click on \"Build a Dashboard\", then \"Import dashboard\" and select the file intel_core_and_igpu_telemetry.json under telegraf folder.</p> <p></p> </li> <li> <p>Run the dashboard</p> <p></p> </li> </ol>"}]}